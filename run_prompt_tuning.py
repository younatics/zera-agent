#!/usr/bin/env python3
"""
í”„ë¡¬í”„íŠ¸ ìë™ íŠœë‹ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

Usage:
    python run_prompt_tuning.py --dataset bbh --total_samples 20 --iteration_samples 5 --iterations 10 --model solar --evaluator solar --meta_model solar --output_dir ./results
"""

import argparse
import os
import sys
import logging
from datetime import datetime
from pathlib import Path
import json
import random

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.append(project_root)

from agent.core.prompt_tuner import PromptTuner
from agent.common.api_client import Model
from agent.dataset.mmlu_dataset import MMLUDataset
from agent.dataset.mmlu_pro_dataset import MMLUProDataset
from agent.dataset.cnn_dataset import CNNDataset
from agent.dataset.gsm8k_dataset import GSM8KDataset
from agent.dataset.mbpp_dataset import MBPPDataset
from agent.dataset.xsum_dataset import XSumDataset
from agent.dataset.bbh_dataset import BBHDataset
from agent.dataset.truthfulqa_dataset import TruthfulQADataset
from agent.dataset.hellaswag_dataset import HellaSwagDataset
from agent.dataset.humaneval_dataset import HumanEvalDataset
from agent.dataset.samsum_dataset import SamsumDataset
from agent.dataset.meetingbank_dataset import MeetingBankDataset

def setup_logging(output_dir):
    """ë¡œê¹… ì„¤ì •"""
    log_file = os.path.join(output_dir, f"prompt_tuning_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
    
    # ì½˜ì†”ê³¼ íŒŒì¼ ëª¨ë‘ì— ë¡œê¹…
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"ë¡œê·¸ íŒŒì¼: {log_file}")
    return logger

def load_dataset(dataset_name, total_samples, logger):
    """ë°ì´í„°ì…‹ ë¡œë“œ"""
    logger.info(f"ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {dataset_name}")
    
    test_cases = []
    
    if dataset_name.lower() == "mmlu":
        dataset = MMLUDataset()
        all_subjects_data = dataset.get_all_subjects_data()
        data = []
        for subject_data in all_subjects_data.values():
            data.extend(subject_data["validation"])
        
        for item in data:
            choices_str = "\n".join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(item['choices'])])
            question = f"{item['question']}\n\nChoices:\n{choices_str}"
            expected = chr(65 + item['answer']) if isinstance(item['answer'], int) else item['answer']
            test_cases.append({
                'question': question,
                'expected': expected
            })
    
    elif dataset_name.lower() == "mmlu_pro":
        dataset = MMLUProDataset()
        all_subjects_data = dataset.get_all_subjects_data()
        data = []
        for subject_data in all_subjects_data.values():
            data.extend(subject_data["validation"])
        
        for item in data:
            choices_str = "\n".join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(item['choices'])])
            question = f"{item['question']}\n\nChoices:\n{choices_str}"
            expected = chr(65 + item['answer']) if isinstance(item['answer'], int) else item['answer']
            test_cases.append({
                'question': question,
                'expected': expected
            })
    
    elif dataset_name.lower() == "bbh":
        dataset = BBHDataset()
        all_data_dict = dataset.get_all_data()
        data = []
        for split_data in all_data_dict.values():
            data.extend(split_data)
        
        for item in data:
            test_cases.append({
                'question': item['input'],
                'expected': item['target']
            })
    
    elif dataset_name.lower() == "cnn":
        dataset = CNNDataset()
        data = dataset.load_all_data("validation")
        
        for item in data:
            normalized_expected = ' '.join(
                line.strip()
                for line in item['expected_answer'].split('\n')
                if line.strip() and not line.strip().startswith(('-', '*'))
            )
            test_cases.append({
                'question': item['input'],
                'expected': normalized_expected
            })
    
    elif dataset_name.lower() == "gsm8k":
        dataset = GSM8KDataset()
        data = dataset.load_data("test")
        
        for item in data:
            test_cases.append({
                'question': item['question'],
                'expected': item['answer']
            })
    
    elif dataset_name.lower() == "mbpp":
        dataset = MBPPDataset()
        data = dataset.get_split_data("test")
        
        for item in data:
            test_cases.append({
                'question': item['text'],
                'expected': item['code']
            })
    
    elif dataset_name.lower() == "xsum":
        dataset = XSumDataset()
        data = dataset.get_split_data("validation")
        
        for item in data:
            test_cases.append({
                'question': item['document'],
                'expected': item['summary']
            })
    
    elif dataset_name.lower() == "truthfulqa":
        dataset = TruthfulQADataset()
        data = dataset.get_split_data("test")
        
        for item in data:
            test_cases.append({
                'question': item['input'],
                'expected': item['target']
            })
    
    elif dataset_name.lower() == "hellaswag":
        dataset = HellaSwagDataset()
        data = dataset.get_split_data("validation")
        
        for item in data:
            choices_str = "\n".join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(item['choices'])])
            question = f"Activity: {item['activity_label']}\nContext: {item['context']}\n\nComplete the context with the most appropriate ending:\n{choices_str}"
            test_cases.append({
                'question': question,
                'expected': chr(65 + item['answer'])
            })
    
    elif dataset_name.lower() == "humaneval":
        dataset = HumanEvalDataset()
        data = dataset.get_split_data("test")
        
        for item in data:
            test_cases.append({
                'question': item['prompt'],
                'expected': item['canonical_solution']
            })
    
    elif dataset_name.lower() == "samsum":
        dataset = SamsumDataset()
        data = dataset.get_split_data("validation")
        
        for item in data:
            test_cases.append({
                'question': item['dialogue'],
                'expected': item['summary']
            })
    
    elif dataset_name.lower() == "meetingbank":
        dataset = MeetingBankDataset()
        data = dataset.get_split_data("validation")
        
        for item in data:
            test_cases.append({
                'question': item['transcript'],
                'expected': item['summary']
            })
    
    else:
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹: {dataset_name}")
    
    # ì „ì²´ ë°ì´í„°ì—ì„œ ìƒ˜í”Œë§
    if total_samples > 0 and total_samples < len(test_cases):
        test_cases = random.sample(test_cases, total_samples)
    
    logger.info(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(test_cases)}ê°œ ìƒ˜í”Œ")
    return test_cases

def save_results(tuner, output_dir, dataset_name, config, logger):
    """ê²°ê³¼ ì €ì¥"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ì„¤ì • ì •ë³´ ì €ì¥
    config_file = os.path.join(output_dir, f"config_{dataset_name}_{timestamp}.json")
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    logger.info(f"ì„¤ì • ì €ì¥: {config_file}")
    
    # ì „ì²´ ê²°ê³¼ CSV ì €ì¥
    csv_data = tuner.save_results_to_csv()
    csv_file = os.path.join(output_dir, f"results_{dataset_name}_{timestamp}.csv")
    with open(csv_file, 'w', encoding='utf-8') as f:
        f.write(csv_data)
    logger.info(f"ì „ì²´ ê²°ê³¼ ì €ì¥: {csv_file}")
    
    # ë¹„ìš© ìš”ì•½ CSV ì €ì¥
    cost_csv_data = tuner.export_cost_summary_to_csv()
    cost_file = os.path.join(output_dir, f"cost_summary_{dataset_name}_{timestamp}.csv")
    with open(cost_file, 'w', encoding='utf-8') as f:
        f.write(cost_csv_data)
    logger.info(f"ë¹„ìš© ìš”ì•½ ì €ì¥: {cost_file}")
    
    # ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ ì €ì¥
    if tuner.iteration_results:
        best_result = max(tuner.iteration_results, key=lambda x: x.avg_score)
        best_prompt_file = os.path.join(output_dir, f"best_prompt_{dataset_name}_{timestamp}.json")
        best_prompt_data = {
            "iteration": best_result.iteration,
            "avg_score": best_result.avg_score,
            "std_dev": best_result.std_dev,
            "top3_avg_score": best_result.top3_avg_score,
            "best_avg_score": best_result.best_avg_score,
            "best_sample_score": best_result.best_sample_score,
            "task_type": best_result.task_type,
            "task_description": best_result.task_description,
            "system_prompt": best_result.system_prompt,
            "user_prompt": best_result.user_prompt,
            "created_at": best_result.created_at.isoformat()
        }
        
        with open(best_prompt_file, 'w', encoding='utf-8') as f:
            json.dump(best_prompt_data, f, ensure_ascii=False, indent=2)
        logger.info(f"ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ ì €ì¥: {best_prompt_file}")
        logger.info(f"ìµœê³  ì„±ëŠ¥: í‰ê·  ì ìˆ˜ {best_result.avg_score:.3f}")

def main():
    parser = argparse.ArgumentParser(description="í”„ë¡¬í”„íŠ¸ ìë™ íŠœë‹ ì‹¤í–‰")
    
    # ë°ì´í„°ì…‹ ì„¤ì •
    parser.add_argument("--dataset", type=str, required=True,
                       choices=["mmlu", "mmlu_pro", "bbh", "cnn", "gsm8k", "mbpp", "xsum", 
                               "truthfulqa", "hellaswag", "humaneval", "samsum", "meetingbank"],
                       help="ì‚¬ìš©í•  ë°ì´í„°ì…‹")
    
    # ìƒ˜í”Œë§ ì„¤ì •
    parser.add_argument("--total_samples", type=int, 
                       choices=[5, 20, 50, 100, 200], default=20,
                       help="ì „ì²´ ë°ì´í„°ì—ì„œ ìƒ˜í”Œë§í•  ê°œìˆ˜ (5, 20, 50, 100, 200)")
    
    parser.add_argument("--iteration_samples", type=int, default=5,
                       help="ë§¤ ì´í„°ë ˆì´ì…˜ë§ˆë‹¤ ì‚¬ìš©í•  ìƒ˜í”Œ ìˆ˜")
    
    parser.add_argument("--iterations", type=int, default=10,
                       help="ì´í„°ë ˆì´ì…˜ ìˆ˜")
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--model", type=str, default="solar",
                       choices=["solar", "gpt4o", "claude", "local1", "local2", "solar_strawberry"],
                       help="ë©”ì¸ ëª¨ë¸")
    
    parser.add_argument("--evaluator", type=str, default="solar",
                       choices=["solar", "gpt4o", "claude", "local1", "local2", "solar_strawberry"],
                       help="í‰ê°€ ëª¨ë¸")
    
    parser.add_argument("--meta_model", type=str, default="solar",
                       choices=["solar", "gpt4o", "claude", "local1", "local2", "solar_strawberry"],
                       help="ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„± ëª¨ë¸")
    
    # íŠœë‹ ì„¤ì •
    parser.add_argument("--use_meta_prompt", action="store_true", default=True,
                       help="ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì—¬ë¶€")
    
    parser.add_argument("--evaluation_threshold", type=float, default=0.8,
                       help="í‰ê°€ í”„ë¡¬í”„íŠ¸ ì ìˆ˜ ì„ê³„ê°’")
    
    parser.add_argument("--score_threshold", type=float, default=None,
                       help="í‰ê·  ì ìˆ˜ ì„ê³„ê°’ (Noneì´ë©´ ì‚¬ìš© ì•ˆí•¨)")
    
    # ì¶œë ¥ ì„¤ì •
    parser.add_argument("--output_dir", type=str, default="./results",
                       help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    
    parser.add_argument("--seed", type=int, default=42,
                       help="ëœë¤ ì‹œë“œ")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ë¡œê¹… ì„¤ì •
    logger = setup_logging(args.output_dir)
    
    # ëœë¤ ì‹œë“œ ì„¤ì •
    random.seed(args.seed)
    logger.info(f"ëœë¤ ì‹œë“œ ì„¤ì •: {args.seed}")
    
    # api_clientì˜ ëª¨ë¸ ì •ë³´ í™œìš©
    from agent.common.api_client import Model
    
    # ì„¤ì • ì •ë³´ (ëª¨ë¸ ë²„ì „ ì •ë³´ ì¶”ê°€)
    config = vars(args).copy()
    config["model_version"] = Model.get_model_info(args.model)["default_version"]
    config["evaluator_version"] = Model.get_model_info(args.evaluator)["default_version"]
    config["meta_model_version"] = Model.get_model_info(args.meta_model)["default_version"]
    
    logger.info("=== í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹œì‘ ===")
    logger.info(f"ì„¤ì •: {json.dumps(config, ensure_ascii=False, indent=2)}")
    
    try:
        # ë°ì´í„°ì…‹ ë¡œë“œ
        test_cases = load_dataset(args.dataset, args.total_samples, logger)
        
        # PromptTuner ì´ˆê¸°í™”
        logger.info("PromptTuner ì´ˆê¸°í™” ì¤‘...")
        tuner = PromptTuner(
            model_name=args.model,
            model_version=config["model_version"],
            evaluator_model_name=args.evaluator,
            evaluator_model_version=config["evaluator_version"],
            meta_prompt_model_name=args.meta_model,
            meta_prompt_model_version=config["meta_model_version"]
        )
        
        # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ
        prompts_dir = os.path.join(os.path.dirname(__file__), 'agent', 'prompts')
        
        with open(os.path.join(prompts_dir, 'initial_system_prompt.txt'), 'r', encoding='utf-8') as f:
            initial_system_prompt = f.read()
        with open(os.path.join(prompts_dir, 'initial_user_prompt.txt'), 'r', encoding='utf-8') as f:
            initial_user_prompt = f.read()
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ì½œë°± ì„¤ì •
        def progress_callback(iteration, test_case_index):
            progress = ((iteration - 1) * args.iteration_samples + test_case_index) / (args.iterations * args.iteration_samples)
            logger.info(f"ì§„í–‰ë„: {progress*100:.1f}% - Iteration {iteration}/{args.iterations}, Test Case {test_case_index}/{args.iteration_samples}")
        
        def iteration_callback(result):
            logger.info(f"Iteration {result.iteration} ì™„ë£Œ - í‰ê·  ì ìˆ˜: {result.avg_score:.3f}, í‘œì¤€í¸ì°¨: {result.std_dev:.3f}")
        
        def best_prompt_callback(iteration, avg_score, system_prompt, user_prompt):
            """ìƒˆë¡œìš´ ë² ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ê°€ ë°œê²¬ë  ë•Œë§ˆë‹¤ ì‹¤ì‹œê°„ìœ¼ë¡œ ì €ì¥"""
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            best_prompt_file = os.path.join(args.output_dir, f"best_prompt_{args.dataset}_{timestamp}.json")
            
            best_prompt_data = {
                "iteration": iteration,
                "avg_score": avg_score,
                "system_prompt": system_prompt,
                "user_prompt": user_prompt,
                "updated_at": datetime.now().isoformat(),
                "note": "ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ëœ ë² ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸"
            }
            
            with open(best_prompt_file, 'w', encoding='utf-8') as f:
                json.dump(best_prompt_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ† ìƒˆë¡œìš´ ë² ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì €ì¥: {best_prompt_file} (Iteration {iteration}, ì ìˆ˜: {avg_score:.3f})")
        
        # í”„ë¡¬í”„íŠ¸ ë³€í™” ê³¼ì • íŠ¸ë™í‚¹ ì½œë°±ë“¤
        def prompt_improvement_start_callback(iteration, avg_score, current_system_prompt, current_user_prompt):
            """í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œì‘ ì‹œì  ì½œë°±"""
            logger.info(f"\nğŸ”„ [Iteration {iteration}] í”„ë¡¬í”„íŠ¸ ê°œì„  ì‹œì‘ (í˜„ì¬ ì ìˆ˜: {avg_score:.3f})")
            logger.info(f"   ğŸ“‹ í˜„ì¬ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {current_system_prompt[:100]}{'...' if len(current_system_prompt) > 100 else ''}")
            logger.info(f"   ğŸ“ í˜„ì¬ ìœ ì € í”„ë¡¬í”„íŠ¸: {current_user_prompt[:100]}{'...' if len(current_user_prompt) > 100 else ''}")
        
        def meta_prompt_generated_callback(iteration, meta_prompt):
            """ë©”íƒ€í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ ì‹œì  ì½œë°±"""
            logger.info(f"\nğŸ“Š [Iteration {iteration}] ë©”íƒ€í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
            logger.info(f"   ğŸ§  ë©”íƒ€í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(meta_prompt)} ë¬¸ì")
            # ë©”íƒ€í”„ë¡¬í”„íŠ¸ì˜ ì¼ë¶€ë§Œ í‘œì‹œ (ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìŒ)
            meta_preview = meta_prompt[:200] + "..." if len(meta_prompt) > 200 else meta_prompt
            logger.info(f"   ğŸ“œ ë©”íƒ€í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {meta_preview}")
        
        def prompt_updated_callback(iteration, previous_system_prompt, previous_user_prompt, 
                                  previous_task_type, previous_task_description,
                                  new_system_prompt, new_user_prompt, 
                                  new_task_type, new_task_description, raw_improved_prompts):
            """í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ ì‹œì  ì½œë°±"""
            logger.info(f"\nâœ¨ [Iteration {iteration}] í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            
            # íƒœìŠ¤í¬ ì •ë³´ ë³€í™”
            if previous_task_type != new_task_type:
                logger.info(f"   ğŸ¯ íƒœìŠ¤í¬ íƒ€ì… ë³€ê²½: '{previous_task_type}' â†’ '{new_task_type}'")
            if previous_task_description != new_task_description:
                logger.info(f"   ğŸ“– íƒœìŠ¤í¬ ì„¤ëª… ë³€ê²½: '{previous_task_description[:50]}...' â†’ '{new_task_description[:50]}...'")
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë³€í™”
            if previous_system_prompt != new_system_prompt:
                logger.info(f"   ğŸ”§ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë³€ê²½:")
                logger.info(f"      ì´ì „: {previous_system_prompt[:100]}{'...' if len(previous_system_prompt) > 100 else ''}")
                logger.info(f"      ì‹ ê·œ: {new_system_prompt[:100]}{'...' if len(new_system_prompt) > 100 else ''}")
            else:
                logger.info(f"   ğŸ”§ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ë³€ê²½ ì—†ìŒ")
            
            # ìœ ì € í”„ë¡¬í”„íŠ¸ ë³€í™”  
            if previous_user_prompt != new_user_prompt:
                logger.info(f"   ğŸ“ ìœ ì € í”„ë¡¬í”„íŠ¸ ë³€ê²½:")
                logger.info(f"      ì´ì „: {previous_user_prompt[:100]}{'...' if len(previous_user_prompt) > 100 else ''}")
                logger.info(f"      ì‹ ê·œ: {new_user_prompt[:100]}{'...' if len(new_user_prompt) > 100 else ''}")
            else:
                logger.info(f"   ğŸ“ ìœ ì € í”„ë¡¬í”„íŠ¸: ë³€ê²½ ì—†ìŒ")
        
        tuner.progress_callback = progress_callback
        tuner.iteration_callback = iteration_callback
        tuner.best_prompt_callback = best_prompt_callback
        tuner.prompt_improvement_start_callback = prompt_improvement_start_callback
        tuner.meta_prompt_generated_callback = meta_prompt_generated_callback
        tuner.prompt_updated_callback = prompt_updated_callback
        
        # í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹¤í–‰
        logger.info("í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹¤í–‰ ì¤‘...")
        results = tuner.tune_prompt(
            initial_system_prompt=initial_system_prompt,
            initial_user_prompt=initial_user_prompt,
            initial_test_cases=test_cases,
            num_iterations=args.iterations,
            score_threshold=args.score_threshold,
            evaluation_score_threshold=args.evaluation_threshold,
            use_meta_prompt=args.use_meta_prompt,
            num_samples=args.iteration_samples
        )
        
        # ê²°ê³¼ ì €ì¥
        logger.info("ê²°ê³¼ ì €ì¥ ì¤‘...")
        save_results(tuner, args.output_dir, args.dataset, config, logger)
        
        # ë¹„ìš© ìš”ì•½ ì¶œë ¥
        cost_summary = tuner.get_cost_summary()
        logger.info("=== ë¹„ìš© ìš”ì•½ ===")
        logger.info(f"ì´ ë¹„ìš©: ${cost_summary['total_cost']:.4f}")
        logger.info(f"ì´ í† í°: {cost_summary['total_tokens']:,}")
        logger.info(f"ì´ ì‹œê°„: {cost_summary['total_duration']:.1f}ì´ˆ")
        logger.info(f"ì´ í˜¸ì¶œ: {cost_summary['total_calls']}")
        
        logger.info("=== í”„ë¡¬í”„íŠ¸ íŠœë‹ ì™„ë£Œ ===")
        
    except Exception as e:
        logger.error(f"í”„ë¡¬í”„íŠ¸ íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main() 