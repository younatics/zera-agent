from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from agent.common.api_client import Model
import json
import time
from pathlib import Path
import logging
from .dataset_loader import DatasetLoader
import os
import random
from agent.common.slack_notify import send_file_to_slack, notify_slack
import subprocess
import sys

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BaseEvaluator(ABC):
    def __init__(self, model_name: str, model_version: str, temperature: float = None, top_p: float = None):
        """
        í‰ê°€ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ì˜ ì´ë¦„
            model_version: ì‚¬ìš©í•  ëª¨ë¸ì˜ ë²„ì „
            temperature: ëª¨ë¸ì˜ temperature ê°’ (ì„ íƒì‚¬í•­)
            top_p: ëª¨ë¸ì˜ top_p ê°’ (ì„ íƒì‚¬í•­)
        """
        self.model_name = model_name
        self.model_version = model_version or "unknown"
        
        # model_versionì´ Noneì´ë©´ Model í´ë˜ìŠ¤ì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš©
        if model_version:
            self.model = Model(model_name).set_version(model_version)
        else:
            self.model = Model(model_name)  # ê¸°ë³¸ ë²„ì „ ì‚¬ìš©
        
        # temperatureì™€ top_pê°€ ì œê³µëœ ê²½ìš°ì—ë§Œ ì„¤ì •
        if temperature is not None:
            self.model.set_temperature(temperature)
            self.temperature = temperature
        if top_p is not None:
            self.model.set_top_p(top_p)
            self.top_p = top_p

        self.results_dir = Path("evaluation/results")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
    def load_dataset(self, dataset_name: str, num_samples: Optional[int] = None) -> List[Dict[str, Any]]:
        """ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ëŠ” ë©”ì„œë“œ"""
        dataset_path = f"data/{dataset_name}/test.json"
        with open(dataset_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            
        if num_samples is not None:
            data = random.sample(data, min(num_samples, len(data)))
            
        return data
        
    @abstractmethod
    def format_question(self, item: Dict[str, Any]) -> str:
        """ê° ë°ì´í„°ì…‹ì˜ ì§ˆë¬¸ì„ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        pass
        
    @abstractmethod
    def evaluate_response(self, response: str, ground_truth: Dict[str, Any]) -> bool:
        """ëª¨ë¸ì˜ ì‘ë‹µì„ í‰ê°€í•˜ëŠ” ë©”ì„œë“œ"""
        pass
        
    def send_slack_notification(self, message: str):
        webhook_url = os.environ.get("SLACK_WEBHOOK_URL")
        if webhook_url:
            notify_slack(message, webhook_url)
        else:
            print("ìŠ¬ë™ ì›¹í›… ì•Œë¦¼ì„ ìœ„í•´ SLACK_WEBHOOK_URL í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    def run_evaluation(self, 
                      dataset_name, 
                      system_prompt: Optional[str] = None,
                      user_prompt: Optional[str] = None,
                      num_samples: Optional[int] = None,
                      sample_indices: Optional[List[int]] = None,
                      is_zera: Optional[bool] = None,
                      num_shots: Optional[int] = None,
                      dataset_display_name: Optional[str] = None) -> Dict[str, Any]:
        """ì „ì²´ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ëŠ” ë©”ì„œë“œ"""
        # --- í‰ê°€ ì‹œì‘ ìŠ¬ë™ ì•Œë¦¼ ---
        model_version = getattr(self, 'model_version', 'unknown')
        if is_zera is True:
            prompt_type = "ğŸ§¬ ì œë¼ í”„ë¡¬í”„íŠ¸"
        elif is_zera is False:
            prompt_type = "ğŸ“ ë² ì´ìŠ¤ í”„ë¡¬í”„íŠ¸"
        else:
            prompt_type = "ğŸ¤– í”„ë¡¬í”„íŠ¸"
        if dataset_display_name:
            dataset_desc = dataset_display_name
        elif isinstance(dataset_name, list):
            dataset_desc = f"Loaded dataset (size={len(dataset_name)})"
        else:
            dataset_desc = str(dataset_name)
        start_msg = f"{prompt_type} í‰ê°€ ì‹œì‘!\nëª¨ë¸ ë²„ì „: {model_version}\në°ì´í„°ì…‹: {dataset_desc}"
        self.send_slack_notification(start_msg)
        # dataset_nameì´ ì´ë¯¸ listë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì•„ë‹ˆë©´ load_dataset í˜¸ì¶œ
        if isinstance(dataset_name, list):
            dataset = dataset_name
        else:
            dataset = self.load_dataset(dataset_name)
        if sample_indices is not None:
            dataset = [dataset[i] for i in sample_indices]
        elif num_samples:
            dataset = random.sample(dataset, min(num_samples, len(dataset)))
        results = {
            "total": len(dataset),
            "correct": 0,
            "samples": [],
            "system_prompt": system_prompt,
            "user_prompt": user_prompt,
            "num_shots": num_shots
        }
        # few-shot ì˜ˆì‹œ ìƒì„±
        few_shot_examples = []
        if num_shots is not None and num_shots > 0:
            # í‰ê°€ì— ì‚¬ìš©ë˜ëŠ” ìƒ˜í”Œê³¼ ê²¹ì¹˜ì§€ ì•Šê²Œ ì˜ˆì‹œë¥¼ ë½‘ìŒ
            available_indices = set(range(len(dataset)))
            if len(dataset) > num_shots:
                few_shot_indices = random.sample(list(available_indices), num_shots)
            else:
                few_shot_indices = list(available_indices)
            for idx in few_shot_indices:
                ex_item = dataset[idx]
                ex_question = self.format_question(ex_item)
                ex_answer = ex_item.get("answer", ex_item)
                few_shot_examples.append(f"[Example {len(few_shot_examples)+1}]\nQuestion: {ex_question}\nAnswer: {ex_answer}\n")
            few_shot_prompt = "\n".join(few_shot_examples)
        else:
            few_shot_prompt = ""
        for idx, item in enumerate(dataset):
            try:
                question = self.format_question(item)
                # user_promptëŠ” ê·¸ëŒ€ë¡œ, question ìœ„ì—ë§Œ few-shot ì˜ˆì‹œ ì¶”ê°€
                full_question = ""
                if few_shot_prompt:
                    full_question += few_shot_prompt + "---\n"  # ì˜ˆì‹œì™€ ì§ˆë¬¸ ì‚¬ì´ì— êµ¬ë¶„ì ì¶”ê°€
                full_question += question
                # ëª¨ë¸ ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° ì œì™¸)
                response_data = self.model.ask(full_question, system_prompt, user_prompt)
                if isinstance(response_data, tuple):
                    response = response_data[0]  # í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì‚¬ìš©
                else:
                    response = response_data  # ì´ë¯¸ í…ìŠ¤íŠ¸ì¸ ê²½ìš°
                
                is_correct = self.evaluate_response(response, item)
                results["correct"] += 1 if is_correct else 0
                sample_info = {
                    "question": full_question,
                    "model_response": response,
                    "actual_answer": item.get("answer", item),
                    "is_correct": is_correct
                }
                results["samples"].append(sample_info)
                print(f"\nìƒ˜í”Œ {idx+1}/{len(dataset)}:")
                print(f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_prompt}")
                print(f"ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸: {user_prompt}")
                print(f"ë¬¸ì œ: {full_question}")
                print(f"ëª¨ë¸ ë‹µë³€: {response}")
                print(f"ì‹¤ì œ ë‹µë³€: {sample_info['actual_answer']}")
                print(f"ì •ë‹µ ì—¬ë¶€: {'ì •ë‹µ' if is_correct else 'ì˜¤ë‹µ'}")
                print("-" * 50)
                logger.info(f"Processed {idx+1}/{len(dataset)} samples")
                time.sleep(1)
            except Exception as e:
                logger.error(f"Error processing sample {idx}: {str(e)}")
                continue
        accuracy = results["correct"] / results["total"] if results["total"] > 0 else 0
        results["accuracy"] = accuracy
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        model_version_safe = (self.model_version or "unknown").replace('/', '_')
        result_file = self.results_dir / f"{self.__class__.__name__}_{model_version_safe}_{timestamp}.json"
        self.save_results(results, str(result_file), is_zera=is_zera)
        return results

    def save_results(self, results: List[Dict[str, Any]], output_path: str, slack_file_upload: bool = True, is_zera: bool = None):
        """ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. slack_file_uploadê°€ Trueë©´ ìŠ¬ë™ ì›¹í›…ìœ¼ë¡œ ê°„ë‹¨í•œ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤."""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        # MBPP í‰ê°€ ê²°ê³¼ë¼ë©´ ì¶”ê°€ ë¶„ì„ íŒŒì¼ ìƒì„±
        if 'MBPPEvaluator' in os.path.basename(output_path):
            try:
                # ë¶„ì„ íŒŒì¼ëª… ê²°ì •
                analysis_path = output_path.replace('.json', '_analysis.json')
                # analyze_mbpp_json_eval.py ì‹¤í–‰
                subprocess.run([
                    sys.executable, 'evaluation/code_analysis/analyze_mbpp_json_eval.py', output_path
                ], check=True)
                print(f"MBPP ì¶”ê°€ ë¶„ì„ íŒŒì¼ ìƒì„±: {analysis_path}")
            except Exception as e:
                print(f"MBPP ë¶„ì„ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        if slack_file_upload:
            model_version = getattr(self, 'model_version', 'unknown')
            accuracy = results.get("accuracy", 'N/A')
            if isinstance(accuracy, float):
                accuracy_str = f"{accuracy:.2%}"
            else:
                accuracy_str = str(accuracy)
            if is_zera is True:
                prompt_type = "ğŸ§¬ ì œë¼ í”„ë¡¬í”„íŠ¸"
            elif is_zera is False:
                prompt_type = "ğŸ“ ë² ì´ìŠ¤ í”„ë¡¬í”„íŠ¸"
            else:
                prompt_type = "ğŸ¤– í”„ë¡¬í”„íŠ¸"
            msg = f"{prompt_type} í‰ê°€ ê²°ê³¼\nëª¨ë¸ ë²„ì „: {model_version}\nì •í™•ë„: {accuracy_str}\nê²°ê³¼ íŒŒì¼: {os.path.basename(output_path)}\nğŸ‰ ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!"
            self.send_slack_notification(msg) 