#!/usr/bin/env python3
"""
ë°°ì¹˜ë¡œ ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹¤í—˜ì„ ì‹¤í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Usage:
    python run_batch_experiments.py --config experiments_config.json
"""

import json
import subprocess
import time
import argparse
import os
from datetime import datetime
import logging
import sys
from pathlib import Path


# í‰ê°€ ì‹œìŠ¤í…œ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent))
from evaluation.base.main import main as evaluation_main

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    log_file = f"batch_experiments_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    # ì»¤ìŠ¤í…€ í¬ë§·í„° í´ë˜ìŠ¤
    class ColoredFormatter(logging.Formatter):
        """ìƒ‰ê¹”ê³¼ ì´ëª¨ì§€ê°€ í¬í•¨ëœ ë¡œê·¸ í¬ë§·í„°"""
        
        # ANSI ìƒ‰ê¹” ì½”ë“œ
        COLORS = {
            'DEBUG': '\033[36m',      # ì²­ë¡ìƒ‰
            'INFO': '\033[32m',       # ë…¹ìƒ‰  
            'WARNING': '\033[33m',    # ë…¸ë€ìƒ‰
            'ERROR': '\033[31m',      # ë¹¨ê°„ìƒ‰
            'CRITICAL': '\033[35m',   # ìì£¼ìƒ‰
            'RESET': '\033[0m'        # ë¦¬ì…‹
        }
        
        # ì´ëª¨ì§€ ë§¤í•‘
        EMOJIS = {
            'DEBUG': 'ğŸ”',
            'INFO': 'ğŸ“',
            'WARNING': 'âš ï¸',
            'ERROR': 'âŒ',
            'CRITICAL': 'ğŸš¨'
        }
        
        def format(self, record):
            # ì‹œê°„ í¬ë§·
            time_str = self.formatTime(record, '%H:%M:%S')
            
            # ë ˆë²¨ë³„ ìƒ‰ê¹”ê³¼ ì´ëª¨ì§€
            level_color = self.COLORS.get(record.levelname, '')
            reset_color = self.COLORS['RESET']
            emoji = self.EMOJIS.get(record.levelname, 'ğŸ“')
            
            # ë©”ì‹œì§€ì— íŠ¹ë³„í•œ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶”ê°€ ì´ëª¨ì§€
            message = record.getMessage()
            if 'ì‹¤í—˜ ì‹œì‘:' in message:
                emoji = 'ğŸš€'
            elif 'ì‹¤í—˜ ì™„ë£Œ:' in message:
                emoji = 'âœ…'
            elif 'ì‹¤í—˜ ì‹¤íŒ¨:' in message:
                emoji = 'âŒ'
            elif 'í‰ê°€ ì‹œì‘' in message:
                emoji = 'ğŸ”'
            elif 'í‰ê°€ ì™„ë£Œ' in message:
                emoji = 'ğŸ¯'
            elif 'ìƒˆë¡œìš´ ë² ìŠ¤íŠ¸' in message:
                emoji = 'ğŸ†'
            elif 'ì§„í–‰ë„:' in message:
                emoji = 'â³'
            elif 'ëŒ€ê¸°' in message:
                emoji = 'â¸ï¸'
            elif 'ë°°ì¹˜ ì‹¤í—˜ ì™„ë£Œ' in message:
                emoji = 'ğŸ‰'
            
            # í¬ë§·ëœ ë¡œê·¸ ë©”ì‹œì§€
            formatted = f"{level_color}{emoji} [{time_str}] {message}{reset_color}"
            return formatted
    
    # ì½˜ì†” í•¸ë“¤ëŸ¬ (ìƒ‰ê¹” ìˆìŒ)
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(ColoredFormatter())
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ (ìƒ‰ê¹” ì—†ìŒ)
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    
    # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

def create_default_config():
    """ê¸°ë³¸ ì‹¤í—˜ ì„¤ì • ìƒì„± - GSM8K ìƒ˜í”Œ ìˆ˜ ë³€í™” ì‹¤í—˜"""
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ì‹¤í–‰ ì‹œì  ê¸°ì¤€)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    config = {
        "experiments": [
            {
                "name": "GSM8K_Sample_5",
                "dataset": "gsm8k",
                "total_samples": 5,
                "iteration_samples": 5,
                "iterations": 10,
                "model": "local1",
                "evaluator": "local1",
                "meta_model": "local1",
                "output_dir": f"./results/gsm8k_sample_5_{timestamp}",
                "enabled": True
            },
            {
                "name": "GSM8K_Sample_20",
                "dataset": "gsm8k",
                "total_samples": 20,
                "iteration_samples": 5,
                "iterations": 10,
                "model": "local1",
                "evaluator": "local1",
                "meta_model": "local1",
                "output_dir": f"./results/gsm8k_sample_20_{timestamp}",
                "enabled": True
            },
            {
                "name": "GSM8K_Sample_50",
                "dataset": "gsm8k",
                "total_samples": 50,
                "iteration_samples": 5,
                "iterations": 10,
                "model": "local1",
                "evaluator": "local1",
                "meta_model": "local1",
                "output_dir": f"./results/gsm8k_sample_50_{timestamp}",
                "enabled": True
            },
            {
                "name": "GSM8K_Sample_100",
                "dataset": "gsm8k",
                "total_samples": 100,
                "iteration_samples": 5,
                "iterations": 10,
                "model": "local1",
                "evaluator": "local1",
                "meta_model": "local1",
                "output_dir": f"./results/gsm8k_sample_100_{timestamp}",
                "enabled": True
            },
            {
                "name": "GSM8K_Sample_200",
                "dataset": "gsm8k",
                "total_samples": 200,
                "iteration_samples": 5,
                "iterations": 10,
                "model": "local1",
                "evaluator": "local1",
                "meta_model": "local1",
                "output_dir": f"./results/gsm8k_sample_200_{timestamp}",
                "enabled": True
            }
        ],
        "global_settings": {
            "use_meta_prompt": True,
            "evaluation_threshold": 0.95,  # ë†’ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê±°ì˜ í•­ìƒ ê°œì„ ë˜ë„ë¡
            "score_threshold": None,
            "seed": 42,
            "delay_between_experiments": 5,  # ì‹¤í—˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            "run_evaluation": True,  # ì‹¤í—˜ ì™„ë£Œ í›„ í‰ê°€ ì‹¤í–‰ ì—¬ë¶€
            "evaluation_samples": 500  # í‰ê°€ìš© ìƒ˜í”Œ ìˆ˜
        }
    }
    return config

def run_evaluation_after_experiment(experiment_config, output_dir, global_settings, logger):
    """ì‹¤í—˜ ì™„ë£Œ í›„ GSM8K í‰ê°€ ì‹¤í–‰"""
    try:
        # í‰ê°€ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
        if not global_settings.get("run_evaluation", True):
            logger.info("í‰ê°€ ì‹¤í–‰ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return True
        
        # ìµœì‹  config_*.json íŒŒì¼ ì°¾ê¸° (ëª¨ë¸ ì •ë³´ë¥¼ ìœ„í•´)
        config_files = list(Path(output_dir).glob("config_*.json"))
        if not config_files:
            logger.warning(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {output_dir}")
            return False
        
        # ê°€ì¥ ìµœì‹  config íŒŒì¼ ì„ íƒ
        latest_config = max(config_files, key=lambda p: p.stat().st_mtime)
        logger.info(f"ì„¤ì • íŒŒì¼ ì‚¬ìš©: {latest_config}")
        
        # config íŒŒì¼ì—ì„œ ëª¨ë¸ ì •ë³´ ë¡œë“œ
        with open(latest_config, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        # ìµœì‹  best_prompt_*.json íŒŒì¼ ì°¾ê¸°
        best_prompt_files = list(Path(output_dir).glob("best_prompt_*.json"))
        if not best_prompt_files:
            logger.warning(f"ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {output_dir}")
            return False
        
        # ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ
        latest_best_prompt = max(best_prompt_files, key=lambda p: p.stat().st_mtime)
        logger.info(f"ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì‚¬ìš©: {latest_best_prompt}")
        
        # ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        with open(latest_best_prompt, 'r', encoding='utf-8') as f:
            best_prompt_data = json.load(f)
        
        system_prompt = best_prompt_data.get('system_prompt', '')
        user_prompt = best_prompt_data.get('user_prompt', '')
        avg_score = best_prompt_data.get('avg_score', 0.0)
        
        # í”„ë¡¬í”„íŠ¸ ìœ íš¨ì„± ê²€ì‚¬
        if not system_prompt or not user_prompt:
            logger.error("í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (íŠœë‹ ì ìˆ˜: {avg_score:.3f})")
        print(f"ğŸ“ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {len(system_prompt)} ë¬¸ì")
        print(f"ğŸ“ ìœ ì € í”„ë¡¬í”„íŠ¸: {len(user_prompt)} ë¬¸ì")
        print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {config_data.get('model', 'solar')} (ë²„ì „: {config_data.get('model_version', 'N/A')})")
        print(f"ğŸ“Š í‰ê°€ ìƒ˜í”Œ ìˆ˜: {global_settings.get('evaluation_samples', 500)}ê°œ")
        
        # api_clientì˜ ëª¨ë¸ ì •ë³´ import
        from agent.common.api_client import Model as ApiModel
        
        # argparse.Namespace ê°ì²´ ìƒì„±
        class EvaluationArgs:
            def __init__(self):
                self.dataset = "gsm8k"
                # config íŒŒì¼ì—ì„œ ëª¨ë¸ ì •ë³´ ì‚¬ìš©
                self.model = config_data.get("model", "local1")
                
                # configì—ì„œ model_versionì„ ê°€ì ¸ì˜¤ê³ , ì—†ìœ¼ë©´ api_clientì˜ ê¸°ë³¸ê°’ ì‚¬ìš©
                model_name = config_data.get("model", "local1")
                self.model_version = config_data.get("model_version") or ApiModel.get_model_info(model_name)["default_version"]
                
                self.base_system_prompt = None
                self.base_user_prompt = None
                self.zera_system_prompt = system_prompt
                self.zera_user_prompt = user_prompt
                self.num_samples = global_settings.get("evaluation_samples", 500)
                self.temperature = 0.7
                self.top_p = 0.9
                self.base_num_shots = 0
                self.zera_num_shots = 0
                self.bbh_category = None
        
        eval_args = EvaluationArgs()
        
        # í‰ê°€ ì‹¤í–‰ ë° ê²°ê³¼ ìº¡ì²˜
        print(f"ğŸ” GSM8K í‰ê°€ ì‹¤í–‰ ì¤‘... (ìƒ˜í”Œ ìˆ˜: {eval_args.num_samples}ê°œ)")
        
        # í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ìº¡ì²˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        eval_output_file = os.path.join(output_dir, f"evaluation_output_{timestamp}.txt")
        
        # ì›ë˜ sys.argvì™€ stdout ë°±ì—…
        original_argv = sys.argv.copy()
        original_stdout = sys.stdout
        
        try:
            # stdoutì„ íŒŒì¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
            with open(eval_output_file, 'w', encoding='utf-8') as f:
                sys.stdout = f
                # evaluation_main í•¨ìˆ˜ í˜¸ì¶œ
                evaluation_main(eval_args)
            
            # í‰ê°€ ê²°ê³¼ íŒŒì¼ì—ì„œ ì •í™•ë„ ì¶”ì¶œ
            accuracy = extract_accuracy_from_output(eval_output_file)
            
            print(f"ğŸ¯ GSM8K í‰ê°€ ì™„ë£Œ!")
            print(f"ğŸ“Š ì •í™•ë„: {accuracy:.2%}")
            print(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼: {eval_output_file}")
            
            # í‰ê°€ ìš”ì•½ ì •ë³´ ì €ì¥
            eval_summary = {
                "experiment_name": experiment_config["name"],
                "tuning_avg_score": avg_score,
                "gsm8k_accuracy": accuracy,
                "evaluation_samples": eval_args.num_samples,
                "timestamp": timestamp,
                "best_prompt_file": str(latest_best_prompt),
                "output_file": eval_output_file
            }
            
            eval_summary_file = os.path.join(output_dir, f"evaluation_summary_{timestamp}.json")
            with open(eval_summary_file, 'w', encoding='utf-8') as f:
                json.dump(eval_summary, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“‹ ìš”ì•½ ì €ì¥: {eval_summary_file}")
            return True
            
        except Exception as e:
            logger.error(f"í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
        finally:
            # stdout ë³µì›
            sys.stdout = original_stdout
            # sys.argv ë³µì›
            sys.argv = original_argv
            
    except Exception as e:
        logger.error(f"í‰ê°€ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def extract_accuracy_from_output(output_file):
    """í‰ê°€ ê²°ê³¼ íŒŒì¼ì—ì„œ ì •í™•ë„ ì¶”ì¶œ"""
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # "ì •í™•ë„: XX.XX%" íŒ¨í„´ ì°¾ê¸°
        import re
        accuracy_match = re.search(r'ì •í™•ë„:\s*([\d.]+)%', content)
        if accuracy_match:
            return float(accuracy_match.group(1)) / 100.0
        
        # "ì œë¼ í”„ë¡¬í”„íŠ¸ ì •í™•ë„: XX.XX%" íŒ¨í„´ ì°¾ê¸°
        zera_accuracy_match = re.search(r'ì œë¼ í”„ë¡¬í”„íŠ¸.*?ì •í™•ë„:\s*([\d.]+)%', content)
        if zera_accuracy_match:
            return float(zera_accuracy_match.group(1)) / 100.0
            
        return 0.0
        
    except Exception as e:
        print(f"ì •í™•ë„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return 0.0

def print_experiment_header(experiment_name, experiment_num, total_experiments, logger):
    """ì‹¤í—˜ ì‹œì‘ í—¤ë” ì¶œë ¥"""
    print("\n" + "="*80)
    print(f"ğŸš€ ì‹¤í—˜ {experiment_num}/{total_experiments}: {experiment_name}")
    print("="*80)

def print_section_divider(title, logger):
    """ì„¹ì…˜ êµ¬ë¶„ì„  ì¶œë ¥"""
    print(f"\n{'â”€'*60}")
    print(f"ğŸ”¸ {title}")
    print("â”€"*60)

def run_experiment(experiment_config, global_settings, logger):
    """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
    experiment_name = experiment_config["name"]
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = experiment_config["output_dir"]
    os.makedirs(output_dir, exist_ok=True)
    
    # ëª…ë ¹ì–´ êµ¬ì„±
    cmd = [
        "python3", "run_prompt_tuning.py",
        "--dataset", experiment_config["dataset"],
        "--total_samples", str(experiment_config["total_samples"]),
        "--iteration_samples", str(experiment_config["iteration_samples"]),
        "--iterations", str(experiment_config["iterations"]),
        "--model", experiment_config["model"],
        "--evaluator", experiment_config["evaluator"],
        "--meta_model", experiment_config["meta_model"],
        "--output_dir", output_dir,
        "--seed", str(global_settings["seed"])
    ]
    
    # ê¸€ë¡œë²Œ ì„¤ì • ì¶”ê°€
    if global_settings["use_meta_prompt"]:
        cmd.append("--use_meta_prompt")
    
    cmd.extend(["--evaluation_threshold", str(global_settings["evaluation_threshold"])])
    
    if global_settings["score_threshold"] is not None:
        cmd.extend(["--score_threshold", str(global_settings["score_threshold"])])
    
    logger.info(f"ì‹¤í–‰ ëª…ë ¹ì–´: {' '.join(cmd)}")
    
    print_section_divider("í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹¤í–‰", logger)
    print(f"ğŸ”¥ ëª…ë ¹ì–´: {' '.join(cmd)}")
    
    # ì‹¤í—˜ ì‹¤í–‰
    start_time = time.time()
    try:
        print("ğŸš€ í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹œì‘...")
        print("â”€" * 60)
        print("ğŸ“ ì‹¤ì‹œê°„ ë¡œê·¸:")
        print("â”€" * 60)
        
        # ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•´ capture_output=Falseë¡œ ë³€ê²½
        result = subprocess.run(cmd, text=True, encoding='utf-8')
        end_time = time.time()
        duration = end_time - start_time
        
        print("â”€" * 60)
        if result.returncode == 0:
            print(f"âœ… í”„ë¡¬í”„íŠ¸ íŠœë‹ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ)")
            
            # ì‹¤í—˜ ì„±ê³µ ì‹œ í‰ê°€ ì‹¤í–‰
            print_section_divider("GSM8K í‰ê°€ ì‹¤í–‰", logger)
            eval_success = run_evaluation_after_experiment(experiment_config, output_dir, global_settings, logger)
            if eval_success:
                print("ğŸ¯ í‰ê°€ ì™„ë£Œ!")
            else:
                print("âš ï¸ í‰ê°€ ì‹¤íŒ¨")
                
        else:
            print(f"âŒ í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹¤íŒ¨! (ë¦¬í„´ ì½”ë“œ: {result.returncode})")
            
        return result.returncode == 0, duration
        
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        print(f"âŒ ì‹¤í—˜ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        logger.error(f"ì˜ˆì™¸ ìƒì„¸: {str(e)}")
        return False, duration

def main():
    parser = argparse.ArgumentParser(description="ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹¤í—˜ ì‹¤í–‰")
    parser.add_argument("--config", type=str, default="experiments_config.json",
                       help="ì‹¤í—˜ ì„¤ì • JSON íŒŒì¼")
    parser.add_argument("--create_config", action="store_true",
                       help="ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±")
    parser.add_argument("--dry_run", action="store_true",
                       help="ì‹¤ì œ ì‹¤í–‰ ì—†ì´ ëª…ë ¹ì–´ë§Œ ì¶œë ¥")
    
    args = parser.parse_args()
    
    logger = setup_logging()
    
    # ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±
    if args.create_config:
        config = create_default_config()
        with open(args.config, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        logger.info(f"ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±: {args.config}")
        return
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ (ì—†ìœ¼ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)
    if os.path.exists(args.config):
        logger.info(f"ì„¤ì • íŒŒì¼ ì‚¬ìš©: {args.config}")
        with open(args.config, 'r', encoding='utf-8') as f:
            config = json.load(f)
    else:
        logger.info("ì„¤ì • íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        config = create_default_config()
    
    experiments = config["experiments"]
    global_settings = config["global_settings"]
    
    # í™œì„±í™”ëœ ì‹¤í—˜ë§Œ í•„í„°ë§
    enabled_experiments = [exp for exp in experiments if exp.get("enabled", True)]
    
    logger.info(f"ì´ {len(enabled_experiments)}ê°œì˜ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    if args.dry_run:
        logger.info("=== DRY RUN MODE ===")
        for i, experiment in enumerate(enabled_experiments):
            logger.info(f"ì‹¤í—˜ {i+1}: {experiment['name']}")
            logger.info(f"  ë°ì´í„°ì…‹: {experiment['dataset']}")
            logger.info(f"  ì „ì²´ ìƒ˜í”Œ: {experiment['total_samples']}")
            logger.info(f"  ì´í„°ë ˆì´ì…˜ ìƒ˜í”Œ: {experiment['iteration_samples']}")
            logger.info(f"  ì´í„°ë ˆì´ì…˜: {experiment['iterations']}")
            logger.info(f"  ëª¨ë¸: {experiment['model']}")
            logger.info(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {experiment['output_dir']}")
        return
    
    # ë°°ì¹˜ ì‹¤í—˜ ì‹œì‘ í—¤ë”
    print("\n" + "ğŸ¯" + "="*78 + "ğŸ¯")
    print(f"ğŸ¯ ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹¤í—˜ ì‹œì‘! ({len(enabled_experiments)}ê°œ ì‹¤í—˜)")
    print("ğŸ¯" + "="*78 + "ğŸ¯")
    
    # ì‹¤í—˜ ì‹¤í–‰
    total_start_time = time.time()
    successful_experiments = 0
    failed_experiments = 0
    
    for i, experiment in enumerate(enabled_experiments):
        # ì‹¤í—˜ í—¤ë” ì¶œë ¥
        print_experiment_header(experiment['name'], i+1, len(enabled_experiments), logger)
        
        # ì‹¤í—˜ ì„¤ì • ìš”ì•½ ì¶œë ¥
        print(f"ğŸ“Š ë°ì´í„°ì…‹: {experiment['dataset']}")
        print(f"ğŸ“ˆ ì´ ìƒ˜í”Œ: {experiment['total_samples']}")
        print(f"ğŸ”„ ì´í„°ë ˆì´ì…˜: {experiment['iterations']}")
        print(f"ğŸ¤– ëª¨ë¸: {experiment['model']}")
        print(f"ğŸ“ ì¶œë ¥: {experiment['output_dir']}")
        
        progress_percent = ((i) / len(enabled_experiments)) * 100
        progress_bar = "â–ˆ" * int(progress_percent // 5) + "â–‘" * (20 - int(progress_percent // 5))
        print(f"\nì „ì²´ ì§„í–‰ë¥ : [{progress_bar}] {progress_percent:.1f}%")
        
        success, duration = run_experiment(experiment, global_settings, logger)
        
        if success:
            successful_experiments += 1
            print(f"\nâœ… ì‹¤í—˜ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ)")
        else:
            failed_experiments += 1
            print(f"\nâŒ ì‹¤í—˜ ì‹¤íŒ¨! (ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ)")
        
        # ë‹¤ìŒ ì‹¤í—˜ ì „ ëŒ€ê¸° (ë§ˆì§€ë§‰ ì‹¤í—˜ì´ ì•„ë‹Œ ê²½ìš°)
        if i < len(enabled_experiments) - 1:
            delay = global_settings.get("delay_between_experiments", 60)
            print(f"\nâ¸ï¸ ë‹¤ìŒ ì‹¤í—˜ê¹Œì§€ {delay}ì´ˆ ëŒ€ê¸°...")
            
            # ì¹´ìš´íŠ¸ë‹¤ìš´ í‘œì‹œ
            for remaining in range(delay, 0, -1):
                print(f"\râ³ ëŒ€ê¸° ì¤‘... {remaining}ì´ˆ ë‚¨ìŒ", end="", flush=True)
                time.sleep(1)
            print("\r" + " " * 30 + "\r", end="")  # ë¼ì¸ í´ë¦¬ì–´
    
    total_duration = time.time() - total_start_time
    
    # ìµœì¢… ìš”ì•½ í—¤ë”
    print("\n" + "ğŸ‰" + "="*78 + "ğŸ‰")
    print("ğŸ‰ ë°°ì¹˜ ì‹¤í—˜ ì™„ë£Œ!")
    print("ğŸ‰" + "="*78 + "ğŸ‰")
    
    # ì‹¤í–‰ ì‹œê°„ í¬ë§·íŒ…
    hours = int(total_duration // 3600)
    minutes = int((total_duration % 3600) // 60)
    seconds = int(total_duration % 60)
    
    if hours > 0:
        time_str = f"{hours}ì‹œê°„ {minutes}ë¶„ {seconds}ì´ˆ"
    elif minutes > 0:
        time_str = f"{minutes}ë¶„ {seconds}ì´ˆ"
    else:
        time_str = f"{seconds}ì´ˆ"
    
    # ìµœì¢… ìš”ì•½
    print(f"â±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {time_str}")
    print(f"âœ… ì„±ê³µí•œ ì‹¤í—˜: {successful_experiments}")
    print(f"âŒ ì‹¤íŒ¨í•œ ì‹¤í—˜: {failed_experiments}")
    print(f"ğŸ“Š ì „ì²´ ì‹¤í—˜: {len(enabled_experiments)}")
    
    # ì„±ê³µë¥  ê³„ì‚° ë° í‘œì‹œ
    if len(enabled_experiments) > 0:
        success_rate = (successful_experiments / len(enabled_experiments)) * 100
        success_bar = "â–ˆ" * int(success_rate // 5) + "â–‘" * (20 - int(success_rate // 5))
        print(f"ğŸ“ˆ ì„±ê³µë¥ : [{success_bar}] {success_rate:.1f}%")
    
    # í‰ê°€ ê²°ê³¼ ìš”ì•½ ìƒì„±
    if global_settings.get("run_evaluation", True):
        print_section_divider("í‰ê°€ ê²°ê³¼ ìš”ì•½", logger)
        generate_evaluation_summary(enabled_experiments, logger)

def generate_evaluation_summary(experiments, logger):
    """ëª¨ë“  ì‹¤í—˜ì˜ í‰ê°€ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
    summary_data = []
    
    for experiment in experiments:
        output_dir = experiment["output_dir"]
        
        # í‰ê°€ ìš”ì•½ íŒŒì¼ ì°¾ê¸°
        eval_summary_files = list(Path(output_dir).glob("evaluation_summary_*.json"))
        if eval_summary_files:
            # ê°€ì¥ ìµœì‹  íŒŒì¼ ì„ íƒ
            latest_summary = max(eval_summary_files, key=lambda p: p.stat().st_mtime)
            
            try:
                with open(latest_summary, 'r', encoding='utf-8') as f:
                    eval_data = json.load(f)
                summary_data.append(eval_data)
            except Exception as e:
                logger.warning(f"í‰ê°€ ìš”ì•½ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {latest_summary} - {str(e)}")
    
    if summary_data:
        # ê²°ê³¼ ì •ë ¬ (GSM8K ì •í™•ë„ ê¸°ì¤€)
        summary_data.sort(key=lambda x: x.get("gsm8k_accuracy", 0), reverse=True)
        
        print("\nğŸ† ì‹¤í—˜ë³„ ì„±ëŠ¥ ìˆœìœ„:")
        print("â”€" * 80)
        print(f"{'ìˆœìœ„':<4} {'ì‹¤í—˜ëª…':<20} {'íŠœë‹ì ìˆ˜':<12} {'GSM8Kì •í™•ë„':<15} {'ìƒ˜í”Œìˆ˜':<8}")
        print("â”€" * 80)
        
        for i, data in enumerate(summary_data, 1):
            exp_name = data.get("experiment_name", "N/A")[:18]
            tuning_score = data.get("tuning_avg_score", 0)
            gsm8k_acc = data.get("gsm8k_accuracy", 0)
            samples = data.get("evaluation_samples", 0)
            
            # ìˆœìœ„ë³„ ë©”ë‹¬ ì´ëª¨ì§€
            if i == 1:
                rank_emoji = "ğŸ¥‡"
            elif i == 2:
                rank_emoji = "ğŸ¥ˆ"
            elif i == 3:
                rank_emoji = "ğŸ¥‰"
            else:
                rank_emoji = f"{i:2d}"
            
            print(f"{rank_emoji:<4} {exp_name:<20} {tuning_score:<12.3f} {gsm8k_acc:<15.2%} {samples:<8}")
        
        print("â”€" * 80)
        
        # ìµœê³  ì„±ëŠ¥ í•˜ì´ë¼ì´íŠ¸
        best_data = summary_data[0]
        print(f"\nğŸ–ï¸  ìµœê³  ì„±ëŠ¥: {best_data.get('experiment_name', 'N/A')}")
        print(f"   ğŸ“Š GSM8K ì •í™•ë„: {best_data.get('gsm8k_accuracy', 0):.2%}")
        print(f"   ğŸ¯ íŠœë‹ ì ìˆ˜: {best_data.get('tuning_avg_score', 0):.3f}")
        
        # ì „ì²´ ìš”ì•½ CSV ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # results í´ë” í™•ì¸/ìƒì„±
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        summary_csv_file = results_dir / f"batch_evaluation_summary_{timestamp}.csv"
        
        try:
            import pandas as pd
            df = pd.DataFrame(summary_data)
            df.to_csv(summary_csv_file, index=False, encoding='utf-8')
            print(f"\nğŸ’¾ ì „ì²´ í‰ê°€ ìš”ì•½ CSV ì €ì¥: {summary_csv_file}")
        except ImportError:
            logger.warning("âš ï¸ pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ CSV íŒŒì¼ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"âŒ CSV íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
    else:
        print("âš ï¸ í‰ê°€ ê²°ê³¼ ìš”ì•½ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 