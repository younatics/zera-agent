import streamlit as st
import pandas as pd
import plotly.express as px
from agent.core.prompt_tuner import PromptTuner
from agent.common.api_client import Model
import os
import logging
import plotly.graph_objects as go
import sys
from dotenv import load_dotenv
import tempfile
import base64
import zipfile
import io
import time
from typing import List, Dict
import numpy as np
from datetime import datetime
import json
from pathlib import Path

# set_page_config must be the first Streamlit command
st.set_page_config(page_title="Prompt Auto Tuning Agent", layout="wide")

def setup_environment():
    # Check if environment is already loaded
    if hasattr(setup_environment, 'loaded'):
        return
    
    # Try to load from different possible locations
    env_paths = [
        '.env',  # Current directory
        '../.env',  # Parent directory
        '../../.env',  # Parent's parent directory
        os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env'),  # Project root
        str(Path.home() / '.env'),  # User's home directory
    ]
    
    env_loaded = False
    for env_path in env_paths:
        if os.path.exists(env_path):
            load_dotenv(env_path, override=True)
            print(f"Loaded environment from: {env_path}")
            env_loaded = True
            break
    
    if not env_loaded:
        print("Warning: No .env file found")
    
    # Verify required environment variables
    required_vars = ['OPENAI_API_KEY']
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        st.error(f"Error: Missing required environment variables: {', '.join(missing_vars)}")
        st.error("Please ensure these variables are set in your .env file or environment")
        st.stop()
    
    # Mark that environment is loaded
    setup_environment.loaded = True

# Call setup at the start
setup_environment()

# Add project root directory to Python path
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from agent.dataset.mmlu_dataset import MMLUDataset
from agent.dataset.mmlu_pro_dataset import MMLUProDataset
from agent.dataset.cnn_dataset import CNNDataset
from agent.dataset.gsm8k_dataset import GSM8KDataset
from agent.dataset.mbpp_dataset import MBPPDataset
from agent.dataset.xsum_dataset import XSumDataset
from agent.dataset.bbh_dataset import BBHDataset
from agent.dataset.truthfulqa_dataset import TruthfulQADataset
from agent.dataset.hellaswag_dataset import HellaSwagDataset
from agent.dataset.humaneval_dataset import HumanEvalDataset
from agent.dataset.samsum_dataset import SamsumDataset
from agent.dataset.meetingbank_dataset import MeetingBankDataset

# Logging configuration
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.title("Prompt Tuning Dashboard")

# Define model information
MODEL_INFO = Model.get_all_model_info()

# Load prompt files
prompts_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'prompts')
with open(os.path.join(prompts_dir, 'initial_system_prompt.txt'), 'r', encoding='utf-8') as f:
    DEFAULT_SYSTEM_PROMPT = f.read()
with open(os.path.join(prompts_dir, 'initial_user_prompt.txt'), 'r', encoding='utf-8') as f:
    DEFAULT_USER_PROMPT = f.read()
with open(os.path.join(prompts_dir, 'evaluation_system_prompt.txt'), 'r', encoding='utf-8') as f:
    DEFAULT_EVALUATION_SYSTEM_PROMPT = f.read()
with open(os.path.join(prompts_dir, 'evaluation_user_prompt.txt'), 'r', encoding='utf-8') as f:
    DEFAULT_EVALUATION_USER_PROMPT = f.read()
with open(os.path.join(prompts_dir, 'meta_system_prompt.txt'), 'r', encoding='utf-8') as f:
    DEFAULT_META_SYSTEM_PROMPT = f.read()
with open(os.path.join(prompts_dir, 'meta_user_prompt.txt'), 'r', encoding='utf-8') as f:
    DEFAULT_META_USER_PROMPT = f.read()

# Create MMLU dataset instance
mmlu_dataset = MMLUDataset()
# Create MMLU Pro dataset instance
mmlu_pro_dataset = MMLUProDataset()

# Create HellaSwag dataset instance
hellaswag_dataset = HellaSwagDataset()

# Create HumanEval dataset instance
humaneval_dataset = HumanEvalDataset()

# Create XSum dataset instance (create only once)
xsum_dataset = XSumDataset()
# Create BBH dataset instance (create only once)
bbh_dataset = BBHDataset()
# Create TruthfulQA dataset instance (create only once)
truthfulqa_dataset = TruthfulQADataset()

# ì‚¬ì´ë“œë°”ì—ì„œ íŒŒë¼ë¯¸í„° ì„¤ì •
with st.sidebar:
    st.header("íŠœë‹ ì„¤ì •")
    
    # ë°˜ë³µ ì„¤ì • ê·¸ë£¹
    with st.expander("ë°˜ë³µ ì„¤ì •", expanded=True):
        iterations = st.slider(
            "ë°˜ë³µ íšŸìˆ˜", 
            min_value=1, 
            max_value=100, 
            value=3,
            help="í”„ë¡¬í”„íŠ¸ íŠœë‹ì„ ìˆ˜í–‰í•  ë°˜ë³µ íšŸìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
        )
    
    # í”„ë¡¬í”„íŠ¸ ê°œì„  ì„¤ì • ê·¸ë£¹
    with st.expander("í”„ë¡¬í”„íŠ¸ ê°œì„  ì„¤ì •", expanded=True):
        # í”„ë¡¬í”„íŠ¸ ê°œì„  ì‚¬ìš© í† ê¸€
        use_meta_prompt = st.toggle(
            "í”„ë¡¬í”„íŠ¸ ê°œì„  ì‚¬ìš©", 
            value=True, 
            help="ë©”íƒ€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•©ë‹ˆë‹¤. ë¹„í™œì„±í™”í•˜ë©´ ì´ˆê¸° í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )
        
        # í‰ê°€ í”„ë¡¬í”„íŠ¸ ì ìˆ˜ ì„ê³„ê°’ ì„¤ì • (í”„ë¡¬í”„íŠ¸ ê°œì„ ì´ ì¼œì ¸ìˆì„ ë•Œë§Œ í™œì„±í™”)
        evaluation_threshold = st.slider(
            "í‰ê°€ í”„ë¡¬í”„íŠ¸ ì ìˆ˜ ì„ê³„ê°’",
            min_value=0.0,
            max_value=1.0,
            value=0.8,
            step=0.1,
            disabled=not use_meta_prompt,
            help="ì´ ì ìˆ˜ ë¯¸ë§Œì´ë©´ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ê°œì„ ì´ ì¼œì ¸ìˆì„ ë•Œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        )
        
        # í‰ê·  ì ìˆ˜ ì„ê³„ê°’ ì ìš© ì—¬ë¶€ í† ê¸€ (í”„ë¡¬í”„íŠ¸ ê°œì„ ì´ ì¼œì ¸ìˆì„ ë•Œë§Œ í™œì„±í™”)
        use_threshold = st.toggle(
            "í‰ê·  ì ìˆ˜ ì„ê³„ê°’ ì ìš©",
            value=True,
            disabled=not use_meta_prompt,
            help="ì´ ì˜µì…˜ì´ ì¼œì ¸ìˆìœ¼ë©´ í‰ê·  ì ìˆ˜ê°€ ì„ê³„ê°’ ì´ìƒì¼ ë•Œ ë°˜ë³µì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ê°œì„ ì´ ì¼œì ¸ìˆì„ ë•Œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        )
        
        # í‰ê·  ì ìˆ˜ ì„ê³„ê°’ ìŠ¬ë¼ì´ë” (í‰ê·  ì ìˆ˜ ì„ê³„ê°’ ì ìš©ì´ êº¼ì ¸ìˆê±°ë‚˜ í”„ë¡¬í”„íŠ¸ ê°œì„ ì´ êº¼ì ¸ìˆì„ ë•ŒëŠ” ë¹„í™œì„±í™”)
        score_threshold = st.slider(
            "í‰ê·  ì ìˆ˜ ì„ê³„ê°’",
            min_value=0.0,
            max_value=1.0,
            value=0.9,
            step=0.05,
            disabled=not (use_threshold and use_meta_prompt),
            help="ì´ ì ìˆ˜ ì´ìƒì´ë©´ ë°˜ë³µì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. í‰ê·  ì ìˆ˜ ì„ê³„ê°’ ì ìš©ê³¼ í”„ë¡¬í”„íŠ¸ ê°œì„ ì´ ëª¨ë‘ ì¼œì ¸ìˆì„ ë•Œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
        )
    
    # ëª¨ë¸ ì„¤ì • ì„¹ì…˜ êµ¬ë¶„ì„ ìœ„í•œ ë””ë°”ì´ë”
    st.divider()
    
    # ëª¨ë¸ ì„¤ì • ê·¸ë£¹
    with st.expander("íŠœë‹ ëª¨ë¸ ì„¤ì •", expanded=True):
        # ëª¨ë¸ ì„ íƒ
        model_name = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            options=list(MODEL_INFO.keys()),
            format_func=lambda x: f"{MODEL_INFO[x]['name']} ({MODEL_INFO[x]['default_version']})",
            index=list(MODEL_INFO.keys()).index("local1") if "local1" in MODEL_INFO else 0,  # local1ì´ ìˆìœ¼ë©´ ê¸°ë³¸ê°’, ì—†ìœ¼ë©´ ì²« ë²ˆì§¸
            help="í”„ë¡¬í”„íŠ¸ íŠœë‹ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”. (solar_strawberry: Upstage Solar-Strawberry API)"
        )
        st.caption(MODEL_INFO[model_name]['description'])
        
        # íŠœë‹ ëª¨ë¸ ë²„ì „ ì„ íƒ
        use_custom_tuning_version = st.toggle(
            "ì»¤ìŠ¤í…€ ë²„ì „ ì‚¬ìš©",
            value=False,  # ê¸°ë³¸ê°’ì„ Falseë¡œ ë³€ê²½
            help="íŠœë‹ ëª¨ë¸ì˜ ê¸°ë³¸ ë²„ì „ ëŒ€ì‹  ì»¤ìŠ¤í…€ ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )
        
        if use_custom_tuning_version:
            tuning_model_version = st.text_input(
                "ëª¨ë¸ ë²„ì „",
                value=MODEL_INFO[model_name]['default_version'],
                help="íŠœë‹ì— ì‚¬ìš©í•  ëª¨ë¸ ë²„ì „ì„ ì…ë ¥í•˜ì„¸ìš”."
            )
        else:
            tuning_model_version = None  # ê¸°ë³¸ ë²„ì „ì„ ì‚¬ìš©í•˜ë„ë¡ Noneìœ¼ë¡œ ì„¤ì •
    
    # ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ëª¨ë¸ ì„¤ì • ê·¸ë£¹
    with st.expander("ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ëª¨ë¸ ì„¤ì •", expanded=True):
        # ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ëª¨ë¸ ì„ íƒ
        meta_prompt_model = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            options=list(MODEL_INFO.keys()),
            format_func=lambda x: f"{MODEL_INFO[x]['name']} ({MODEL_INFO[x]['default_version']})",
            # index=list(MODEL_INFO.keys()).index("local1") if "local1" in MODEL_INFO else 0,
            help="ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”. (solar_strawberry: Upstage Solar-Strawberry API)"
        )
        st.caption(MODEL_INFO[meta_prompt_model]['description'])
        
        # ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ëª¨ë¸ ë²„ì „ ì„ íƒ
        use_custom_meta_version = st.toggle(
            "ì»¤ìŠ¤í…€ ë²„ì „ ì‚¬ìš©",
            value=False,  # ê¸°ë³¸ê°’ì„ Falseë¡œ ë³€ê²½
            help="ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ëª¨ë¸ì˜ ê¸°ë³¸ ë²„ì „ ëŒ€ì‹  ì»¤ìŠ¤í…€ ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )
        
        if use_custom_meta_version:
            meta_model_version = st.text_input(
                "ëª¨ë¸ ë²„ì „",
                value=MODEL_INFO[meta_prompt_model]['default_version'],
                help="ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„±ì— ì‚¬ìš©í•  ëª¨ë¸ ë²„ì „ì„ ì…ë ¥í•˜ì„¸ìš”."
            )
        else:
            meta_model_version = None  # ê¸°ë³¸ ë²„ì „ì„ ì‚¬ìš©í•˜ë„ë¡ Noneìœ¼ë¡œ ì„¤ì •
    
    # í‰ê°€ ëª¨ë¸ ì„¤ì • ê·¸ë£¹
    with st.expander("í‰ê°€ ëª¨ë¸ ì„¤ì •", expanded=True):
        # í‰ê°€ ëª¨ë¸ ì„ íƒ
        evaluator_model = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            options=list(MODEL_INFO.keys()),
            format_func=lambda x: f"{MODEL_INFO[x]['name']} ({MODEL_INFO[x]['default_version']})",
            index=list(MODEL_INFO.keys()).index("local1") if "local1" in MODEL_INFO else 0,
            help="ì¶œë ¥ í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”. (solar_strawberry: Upstage Solar-Strawberry API)"
        )
        st.caption(MODEL_INFO[evaluator_model]['description'])
        
        # í‰ê°€ ëª¨ë¸ ë²„ì „ ì„ íƒ
        use_custom_evaluator_version = st.toggle(
            "ì»¤ìŠ¤í…€ ë²„ì „ ì‚¬ìš©",
            value=False,
            help="í‰ê°€ ëª¨ë¸ì˜ ê¸°ë³¸ ë²„ì „ ëŒ€ì‹  ì»¤ìŠ¤í…€ ë²„ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )
        
        if use_custom_evaluator_version:
            evaluator_model_version = st.text_input(
                "ëª¨ë¸ ë²„ì „",
                value=MODEL_INFO[evaluator_model]['default_version'],
                help="í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸ ë²„ì „ì„ ì…ë ¥í•˜ì„¸ìš”."
            )
        else:
            evaluator_model_version = None  # ê¸°ë³¸ ë²„ì „ì„ ì‚¬ìš©í•˜ë„ë¡ Noneìœ¼ë¡œ ì„¤ì •

# PromptTuner ê°ì²´ ìƒì„±
tuner = PromptTuner(
    model_name=model_name,
    evaluator_model_name=evaluator_model,
    meta_prompt_model_name=meta_prompt_model,
    model_version=tuning_model_version,
    evaluator_model_version=evaluator_model_version,
    meta_prompt_model_version=meta_model_version
)

# í”„ë¡¬í”„íŠ¸ ì„¤ì •
with st.expander("ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì„¤ì •", expanded=False):
    col1, col2 = st.columns(2)
    with col1:
        system_prompt = st.text_area(
            "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
            value=DEFAULT_SYSTEM_PROMPT,
            height=100,
            help="íŠœë‹ì„ ì‹œì‘í•  ì´ˆê¸° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
        )
    with col2:
        user_prompt = st.text_area(
            "ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸",
            value=DEFAULT_USER_PROMPT,
            height=100,
            help="íŠœë‹ì„ ì‹œì‘í•  ì´ˆê¸° ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
        )
    
    if st.button("ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸", key="initial_prompt_update"):
        tuner.set_initial_prompt(system_prompt, user_prompt)
        st.success("ì´ˆê¸° í”„ë¡¬í”„íŠ¸ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë©”íƒ€í”„ë¡¬í”„íŠ¸ ì„¤ì •
with st.expander("ë©”íƒ€í”„ë¡¬í”„íŠ¸ ì„¤ì •", expanded=False):
    col1, col2 = st.columns(2)
    with col1:
        meta_system_prompt = st.text_area(
            "ë©”íƒ€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
            value=DEFAULT_META_SYSTEM_PROMPT,
            height=300,
            help="í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ì˜ ì—­í• ê³¼ ì±…ì„ì„ ì •ì˜í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
        )
    with col2:
        meta_user_prompt = st.text_area(
            "ë©”íƒ€ ìœ ì € í”„ë¡¬í”„íŠ¸",
            value=DEFAULT_META_USER_PROMPT,
            height=300,
            help="í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ ì…ë ¥ ë°ì´í„°ì™€ ì¶œë ¥ í˜•ì‹ì„ ì •ì˜í•˜ëŠ” ìœ ì € í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
        )
    
    if st.button("ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸", key="meta_prompt_update"):
        tuner.set_meta_prompt(meta_system_prompt, meta_user_prompt)
        st.success("ë©”íƒ€ í”„ë¡¬í”„íŠ¸ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

# í‰ê°€ í”„ë¡¬í”„íŠ¸ ì„¤ì •
with st.expander("í‰ê°€ í”„ë¡¬í”„íŠ¸ ì„¤ì •", expanded=False):
    col1, col2 = st.columns(2)
    with col1:
        evaluation_system_prompt = st.text_area(
            "í‰ê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
            value=DEFAULT_EVALUATION_SYSTEM_PROMPT,
            height=200,
            help="í‰ê°€ ëª¨ë¸ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
        )
    with col2:
        evaluation_user_prompt = st.text_area(
            "í‰ê°€ ìœ ì € í”„ë¡¬í”„íŠ¸",
            value=DEFAULT_EVALUATION_USER_PROMPT,
            height=200,
            help="í‰ê°€ ëª¨ë¸ì˜ ìœ ì € í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. {question}, {output}, {expected}ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤."
        )
    
    if st.button("í‰ê°€ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸", key="eval_prompt_update"):
        tuner.set_evaluation_prompt(evaluation_system_prompt, evaluation_user_prompt)
        st.success("í‰ê°€ í”„ë¡¬í”„íŠ¸ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ë°ì´í„°ì…‹ ì²˜ë¦¬ ê³µí†µ í•¨ìˆ˜
def process_dataset(data, dataset_type):
    # ë°ì´í„° í‘œì‹œ
    total_examples = len(data)
    st.write(f"ì´ ì˜ˆì œ ìˆ˜: {total_examples}")
    
    # ìƒ˜í”Œ ìˆ˜ ì„ íƒ
    num_samples = st.slider(
        "Number of random samples to evaluate per iteration",
        min_value=1,
        max_value=min(100, total_examples),  # ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ë¥¼ 100ìœ¼ë¡œ ì œí•œ
        value=min(5, total_examples),
        help="ê° iterationë§ˆë‹¤ í‰ê°€í•  ëœë¤ ìƒ˜í”Œì˜ ê°œìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”."
    )
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± ë° ë°ì´í„°í”„ë ˆì„ ìƒì„±
    test_cases = []
    display_data = []
    
    if dataset_type == "Samsum":
        for item in data:
            test_cases.append({
                'question': item['dialogue'],
                'expected': item['summary']
            })
            if len(display_data) < 2000:
                display_data.append({
                    'question': item['dialogue'],
                    'expected_answer': item['summary']
                })
    elif dataset_type == "MeetingBank":
        for item in data:
            test_cases.append({
                'question': item['transcript'],
                'expected': item['summary']
            })
            if len(display_data) < 2000:
                display_data.append({
                    'question': item['transcript'],
                    'expected_answer': item['summary']
                })
    elif dataset_type == "BBH":
        for item in data:
            test_cases.append({
                'question': item['input'],
                'expected': item['target']
            })
            
            if len(display_data) < 2000:  # display_dataë¥¼ 2000ê°œë¡œ ì œí•œ
                display_data.append({
                    'question': item['input'],
                    'expected_answer': item['target']
                })
    elif dataset_type == "MBPP":
        for item in data:
            test_cases.append({
                'question': item['text'],
                'expected': item['code']
            })
            
            if len(display_data) < 2000:  # display_dataë¥¼ 2000ê°œë¡œ ì œí•œ
                display_data.append({
                    'question': item['text'],
                    'expected_answer': f"```python\n{item['code']}\n```"
                })
    elif dataset_type == "XSum":
        for item in data:
            test_cases.append({
                'question': item['document'],
                'expected': item['summary']
            })
            
            if len(display_data) < 2000:  # display_dataë¥¼ 2000ê°œë¡œ ì œí•œ
                display_data.append({
                    'question': item['document'],  # ì²˜ìŒ 200ìë§Œ í‘œì‹œ
                    'expected_answer': item['summary']
                })
    elif dataset_type in ["MMLU", "MMLU Pro"]:
        for item in data:
            # ì„ íƒì§€ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            choices_str = "\n".join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(item['choices'])])
            question = f"{item['question']}\n\nChoices:\n{choices_str}"
            # answer íƒ€ì…ì— ë”°ë¼ expected ì²˜ë¦¬
            if isinstance(item['answer'], int):
                expected = chr(65 + item['answer'])
            elif isinstance(item['answer'], str) and len(item['answer']) == 1 and item['answer'].isalpha():
                expected = item['answer']
            else:
                expected = item['answer']  # í•´ì„¤ ë“± ê¸°íƒ€ ë¬¸ìì—´
            test_cases.append({
                'question': question,
                'expected': expected
            })
            if len(display_data) < 2000:  # display_dataë¥¼ 2000ê°œë¡œ ì œí•œ
                display_data.append({
                    'question': question,
                    'expected_answer': expected
                })
    elif dataset_type == "CSV":
        # ì»¬ëŸ¼ ì´ë¦„ í™•ì¸ ë° ë§¤í•‘
        required_columns = ['question', 'expected_answer']
        available_columns = data.columns.tolist()
        
        # í•„ìˆ˜ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        missing_columns = [col for col in required_columns if col not in available_columns]
        if missing_columns:
            st.error(f"CSV íŒŒì¼ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤: {', '.join(missing_columns)}")
            st.info("CSV íŒŒì¼ì€ 'question'ê³¼ 'expected_answer' ì»¬ëŸ¼ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.")
            st.stop()
        
        for _, row in data.iterrows():
            test_cases.append({
                'question': row['question'],
                'expected': row['expected_answer']
            })
            
            if len(display_data) < 2000:  # display_dataë¥¼ 2000ê°œë¡œ ì œí•œ
                display_data.append({
                    'question': row['question'],
                    'expected_answer': row['expected_answer']
                })
    elif dataset_type == "CNN":
        for item in data:
            normalized_expected = ' '.join(
                line.strip()
                for line in item['expected_answer'].split('\n')
                if line.strip() and not line.strip().startswith(('-', '*'))
            )
            test_cases.append({
                'question': item['input'],
                'expected': normalized_expected
            })
            
            if len(display_data) < 2000:  # display_dataë¥¼ 2000ê°œë¡œ ì œí•œ
                display_data.append({
                    'question': item['input'],
                    'expected_answer': normalized_expected
                })
    elif dataset_type == "GSM8K":
        for item in data:
            test_cases.append({
                'question': item['question'],
                'expected': item['answer']
            })
            
            if len(display_data) < 2000:  # display_dataë¥¼ 2000ê°œë¡œ ì œí•œ
                display_data.append({
                    'question': item['question'],
                    'expected_answer': item['answer']
                })
    elif dataset_type == "TruthfulQA":
        for item in data:
            test_cases.append({
                'question': item['input'],
                'expected': item['target']
            })
            
            if len(display_data) < 2000:  # display_dataë¥¼ 2000ê°œë¡œ ì œí•œ
                display_data.append({
                    'question': item['input'],
                    'expected_answer': item['target']
                })
    elif dataset_type == "HellaSwag":
        for item in data:
            # ì„ íƒì§€ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            choices_str = "\n".join([f"{chr(65+i)}. {choice}" for i, choice in enumerate(item['choices'])])
            question = f"Activity: {item['activity_label']}\nContext: {item['context']}\n\nComplete the context with the most appropriate ending:\n{choices_str}"
            
            test_cases.append({
                'question': question,
                'expected': chr(65 + item['answer'])  # 0-based indexë¥¼ A, B, C, Dë¡œ ë³€í™˜
            })
            
            if len(display_data) < 2000:  # display_dataë¥¼ 2000ê°œë¡œ ì œí•œ
                display_data.append({
                    'question': question,
                    'expected_answer': chr(65 + item['answer'])
                })
    elif dataset_type == "HumanEval":
        for item in data:
            test_cases.append({
                'question': item['prompt'],
                'expected': item['canonical_solution']
            })
            
            if len(display_data) < 2000:  # display_dataë¥¼ 2000ê°œë¡œ ì œí•œ
                display_data.append({
                    'question': item['prompt'],
                    'expected_answer': item['canonical_solution']
                })
    elif dataset_type in ["MMLU", "MMLU Pro"]:
        # ì„ íƒëœ ë°ì´í„°ì…‹ì— ë”°ë¼ ì ì ˆí•œ ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ì™€ ê³¼ëª© ë¦¬ìŠ¤íŠ¸ ì„ íƒ
        if dataset_type == "MMLU":
            dataset = mmlu_dataset
            dataset_name = "MMLU"
        else:  # MMLU Pro
            dataset = mmlu_pro_dataset
            dataset_name = "MMLU Pro"
        
        # ë°ì´í„°ì…‹ ì„ íƒì— 'ëª¨ë“  ê³¼ëª©' ì˜µì…˜ ì¶”ê°€
        subject_options = ["ëª¨ë“  ê³¼ëª©"] + dataset.subjects
        subject = st.selectbox(
            f"Select {dataset_name} Subject",
            subject_options,
            index=0
        )
        split = st.selectbox(
            "Select Data Split",
            ["validation", "test"],
            index=0
        )
        try:
            if subject == "ëª¨ë“  ê³¼ëª©":
                # ëª¨ë“  ê³¼ëª©ì˜ ë°ì´í„° ë¡œë“œ
                all_subjects_data = dataset.get_all_subjects_data()
                # ëª¨ë“  ê³¼ëª©ì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
                data = []
                for subject_data in all_subjects_data.values():
                    data.extend(subject_data[split])
            else:
                # íŠ¹ì • ê³¼ëª©ì˜ ë°ì´í„° ë¡œë“œ
                subject_data = dataset.get_subject_data(subject)
                data = subject_data[split]
            
            test_cases, num_samples = process_dataset(data, dataset_type)
        except Exception as e:
            st.error(f"{dataset_name} ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            st.stop()

    # ì „ì²´ ë°ì´í„° í‘œì‹œ
    st.write("ë°ì´í„°ì…‹ ë‚´ìš©:")
    st.dataframe(pd.DataFrame(display_data))
    
    return test_cases, num_samples

# ë°ì´í„°ì…‹ ì„ íƒ
st.header("Dataset Selection")
dataset_type = st.radio(
    "Select Dataset Type",
    ["CSV", "MMLU", "MMLU Pro", "CNN", "GSM8K", "MBPP", "BBH", "TruthfulQA", "HellaSwag", "HumanEval", "Samsum", "MeetingBank"],
    horizontal=True
)

if dataset_type == "CSV":
    csv_file = st.file_uploader("Upload CSV file", type=['csv'])
    if csv_file is not None:
        try:
            # CSV íŒŒì¼ì„ ì½ì„ ë•Œ ë” ìœ ì—°í•œ íŒŒì‹± ì˜µì…˜ ì‚¬ìš©
            df = pd.read_csv(csv_file, 
                            encoding='utf-8',
                            on_bad_lines='skip',  # ë¬¸ì œê°€ ìˆëŠ” ì¤„ì€ ê±´ë„ˆë›°ê¸°
                            quoting=1,  # ëª¨ë“  í•„ë“œë¥¼ ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°
                            escapechar='\\')  # ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì„¤ì •
            
            # ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if df.empty:
                st.error("CSV íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ë°ì´í„°ê°€ í¬í•¨ëœ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
                st.stop()
            
            test_cases, num_samples = process_dataset(df, "CSV")
        except Exception as e:
            st.error(f"CSV íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            st.info("CSV íŒŒì¼ì´ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸í•˜ì„¸ìš”. íŒŒì¼ì´ ë¹„ì–´ìˆê±°ë‚˜, ì¸ì½”ë”©ì´ UTF-8ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            st.stop()
    else:
        st.info("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš”.")
        st.stop()
elif dataset_type == "CNN":
    # CNN ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    cnn_dataset = CNNDataset()
    
    # ë°ì´í„°ì…‹ ì„ íƒ
    split = st.selectbox(
        "ë°ì´í„°ì…‹ ì„ íƒ",
        ["train", "validation", "test"],
        index=0
    )
    
    # ì²­í¬ ìˆ˜ í™•ì¸
    total_chunks = cnn_dataset.get_num_chunks(split)
    
    if total_chunks == 0:
        st.error(f"{split} ë°ì´í„°ì…‹ì— ì²­í¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    
    # ì „ì²´ ì²­í¬ ì„ íƒ ì˜µì…˜ ì¶”ê°€
    use_all_chunks = st.toggle(
        "ì „ì²´ ì²­í¬ ì‚¬ìš©",
        value=False,
        help="ëª¨ë“  ì²­í¬ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    
    try:
        if use_all_chunks:
            # ëª¨ë“  ì²­í¬ ë¡œë“œ
            data = cnn_dataset.load_all_data(split)
            test_cases, num_samples = process_dataset(data, "CNN")
            
            # ì„ íƒëœ ì²­í¬ ì •ë³´ í‘œì‹œ
            st.info(f"ì „ì²´ ì²­í¬ ë¡œë“œ ì™„ë£Œ ({len(data):,}ê°œ ì˜ˆì œ)")
        else:
            # ì²­í¬ ì„ íƒ
            st.write(f"ì´ {total_chunks}ê°œì˜ ì²­í¬ê°€ ìˆìŠµë‹ˆë‹¤.")
            chunk_index = st.number_input(
                "ì²­í¬ ì„ íƒ",
                min_value=0,
                max_value=total_chunks-1,
                value=0,
                help="ì²˜ë¦¬í•  ì²­í¬ì˜ ì¸ë±ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”."
            )
            
            # ì„ íƒëœ ì²­í¬ ë¡œë“œ
            data = cnn_dataset.load_data(split, chunk_index)
            test_cases, num_samples = process_dataset(data, "CNN")
            
            # ì„ íƒëœ ì²­í¬ ì •ë³´ í‘œì‹œ
            st.info(f"ì„ íƒëœ ì²­í¬: {chunk_index} ({len(data):,}ê°œ ì˜ˆì œ)")
    except Exception as e:
        st.error(f"CNN ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()
elif dataset_type == "GSM8K":
    # GSM8K ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    gsm8k_dataset = GSM8KDataset()
    
    # ë°ì´í„°ì…‹ ì„ íƒ
    split = st.selectbox(
        "ë°ì´í„°ì…‹ ì„ íƒ",
        ["train", "test"],
        index=0
    )
    
    try:
        # ë°ì´í„° ë¡œë“œ
        data = gsm8k_dataset.load_data(split)
        test_cases, num_samples = process_dataset(data, "GSM8K")
        
        # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
        st.info(f"GSM8K {split} ë°ì´í„°ì…‹: {len(data):,}ê°œ ì˜ˆì œ")
    except Exception as e:
        st.error(f"GSM8K ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()
elif dataset_type == "MBPP":
    # MBPP ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    mbpp_dataset = MBPPDataset()
    
    # ë°ì´í„°ì…‹ ì„ íƒ
    split = st.selectbox(
        "ë°ì´í„°ì…‹ ì„ íƒ",
        ["train", "test", "validation"],
        index=1  # testë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
    )
    
    try:
        # ë°ì´í„° ë¡œë“œ
        data = mbpp_dataset.get_split_data(split)
        test_cases, num_samples = process_dataset(data, "MBPP")
        
        # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
        st.info(f"MBPP {split} ë°ì´í„°ì…‹: {len(data):,}ê°œ ì˜ˆì œ")
    except Exception as e:
        st.error(f"MBPP ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()
elif dataset_type == "BBH":
    # ì´ë¯¸ ìƒì„±ëœ BBHDataset ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©
    try:
        # ì¹´í…Œê³ ë¦¬ ì„ íƒ UIì— "ëª¨ë“  ì¹´í…Œê³ ë¦¬" ì˜µì…˜ ì¶”ê°€
        bbh_categories = ["ëª¨ë“  ì¹´í…Œê³ ë¦¬"] + bbh_dataset.get_all_categories()
        selected_category = st.selectbox(
            "BBH ì¹´í…Œê³ ë¦¬ ì„ íƒ",
            bbh_categories,
            index=0,
            key="bbh_category_selectbox"
        )
        if selected_category == "ëª¨ë“  ì¹´í…Œê³ ë¦¬":
            # ì „ì²´ ë°ì´í„° ë¡œë“œ
            all_data_dict = bbh_dataset.get_all_data()
            # {"test": [...]} í˜•íƒœì´ë¯€ë¡œ í•©ì³ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            data = []
            for split_data in all_data_dict.values():
                data.extend(split_data)
            st.info(f"BBH ì „ì²´ ë°ì´í„°ì…‹: {len(data):,}ê°œ ì˜ˆì œ")
        else:
            # ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ë¡œë“œ
            data = bbh_dataset.get_category_data(selected_category)
            st.info(f"BBH {selected_category} ì¹´í…Œê³ ë¦¬ ë°ì´í„°ì…‹: {len(data):,}ê°œ ì˜ˆì œ")
        test_cases, num_samples = process_dataset(data, "BBH")
    except Exception as e:
        st.error(f"BBH ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()
elif dataset_type == "TruthfulQA":
    # ì´ë¯¸ ìƒì„±ëœ TruthfulQADataset ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©
    try:
        # ë°ì´í„° ë¡œë“œ
        data = truthfulqa_dataset.get_split_data("test")
        test_cases, num_samples = process_dataset(data, "TruthfulQA")
        
        # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
        st.info(f"TruthfulQA í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹: {len(data):,}ê°œ ì˜ˆì œ")
    except Exception as e:
        st.error(f"TruthfulQA ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()
elif dataset_type == "HellaSwag":
    try:
        # ë°ì´í„°ì…‹ ì„ íƒ
        split = st.selectbox(
            "ë°ì´í„°ì…‹ ì„ íƒ",
            ["validation", "train"],
            index=0
        )
        
        # ë°ì´í„° ë¡œë“œ
        data = hellaswag_dataset.get_split_data(split)
        test_cases, num_samples = process_dataset(data, "HellaSwag")
        
        # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
        st.info(f"HellaSwag {split} ë°ì´í„°ì…‹: {len(data):,}ê°œ ì˜ˆì œ")
    except Exception as e:
        st.error(f"HellaSwag ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()
elif dataset_type == "HumanEval":
    try:
        # HumanEvalì€ test splitë§Œ ìˆìŒ
        data = humaneval_dataset.get_split_data("test")
        test_cases, num_samples = process_dataset(data, "HumanEval")
        
        # ë°ì´í„°ì…‹ ì •ë³´ í‘œì‹œ
        st.info(f"HumanEval í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹: {len(data):,}ê°œ ì˜ˆì œ")
    except Exception as e:
        st.error(f"HumanEval ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()
elif dataset_type == "Samsum":
    samsum_dataset = SamsumDataset()
    split = st.selectbox(
        "ë°ì´í„°ì…‹ ì„ íƒ",
        ["train", "validation", "test"],
        index=0
    )
    try:
        data = samsum_dataset.get_split_data(split)
        test_cases, num_samples = process_dataset(data, "Samsum")
        st.info(f"Samsum {split} ë°ì´í„°ì…‹: {len(data):,}ê°œ ì˜ˆì œ")
    except Exception as e:
        st.error(f"Samsum ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()
elif dataset_type == "MeetingBank":
    meetingbank_dataset = MeetingBankDataset()
    split = st.selectbox(
        "ë°ì´í„°ì…‹ ì„ íƒ",
        ["validation", "test"],
        index=0
    )
    try:
        data = meetingbank_dataset.get_split_data(split)
        test_cases, num_samples = process_dataset(data, "MeetingBank")
        st.info(f"MeetingBank {split} ë°ì´í„°ì…‹: {len(data):,}ê°œ ì˜ˆì œ")
    except Exception as e:
        st.error(f"MeetingBank ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()
elif dataset_type in ["MMLU", "MMLU Pro"]:
    # ì„ íƒëœ ë°ì´í„°ì…‹ì— ë”°ë¼ ì ì ˆí•œ ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ì™€ ê³¼ëª© ë¦¬ìŠ¤íŠ¸ ì„ íƒ
    if dataset_type == "MMLU":
        dataset = mmlu_dataset
        dataset_name = "MMLU"
    else:  # MMLU Pro
        dataset = mmlu_pro_dataset
        dataset_name = "MMLU Pro"
    # ë°ì´í„°ì…‹ ì„ íƒì— 'ëª¨ë“  ê³¼ëª©' ì˜µì…˜ ì¶”ê°€
    subject_options = ["ëª¨ë“  ê³¼ëª©"] + dataset.subjects
    subject = st.selectbox(
        f"Select {dataset_name} Subject",
        subject_options,
        index=0
    )
    split = st.selectbox(
        "Select Data Split",
        ["validation", "test"],
        index=0
    )
    try:
        if subject == "ëª¨ë“  ê³¼ëª©":
            # ëª¨ë“  ê³¼ëª©ì˜ ë°ì´í„° ë¡œë“œ
            all_subjects_data = dataset.get_all_subjects_data()
            # ëª¨ë“  ê³¼ëª©ì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
            data = []
            for subject_data in all_subjects_data.values():
                data.extend(subject_data[split])
        else:
            # íŠ¹ì • ê³¼ëª©ì˜ ë°ì´í„° ë¡œë“œ
            subject_data = dataset.get_subject_data(subject)
            data = subject_data[split]
        test_cases, num_samples = process_dataset(data, dataset_type)
    except Exception as e:
        st.error(f"{dataset_name} ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.stop()

class SessionState:
    """
    Streamlit ì•±ì˜ ì„¸ì…˜ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    """
    @staticmethod
    def init_state():
        """ì„¸ì…˜ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        st.session_state.all_iteration_results = []
        st.session_state.current_iteration = 0
        st.session_state.show_results = False
        st.session_state.tuning_complete = False
        st.session_state.display_container = st.empty()
    
    @staticmethod
    def reset():
        """ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        st.session_state.all_iteration_results = []
        st.session_state.current_iteration = 0
        st.session_state.show_results = False
        st.session_state.tuning_complete = False
    
    @staticmethod
    def update_results(result):
        """ìƒˆë¡œìš´ ê²°ê³¼ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        # ë¡œê¹… ì¶”ê°€
        logging.info(f"Updating results for iteration {result.iteration}")
        
        # ê²°ê³¼ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        if not hasattr(st.session_state, 'all_iteration_results'):
            st.session_state.all_iteration_results = []
        
        # ê°™ì€ iterationì˜ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì—…ë°ì´íŠ¸, ì—†ë‹¤ë©´ ì¶”ê°€
        existing_result = next(
            (r for r in st.session_state.all_iteration_results if r.iteration == result.iteration),
            None
        )
        
        if existing_result:
            index = st.session_state.all_iteration_results.index(existing_result)
            st.session_state.all_iteration_results[index] = result
            logging.info(f"Updated existing result at index {index}")
        else:
            st.session_state.all_iteration_results.append(result)
            logging.info(f"Added new result, total results: {len(st.session_state.all_iteration_results)}")
        
        st.session_state.current_iteration = result.iteration - 1  # 0-based index
        st.session_state.show_results = True
        logging.info(f"Session state updated: current_iteration={st.session_state.current_iteration}, show_results={st.session_state.show_results}")
    
    @staticmethod
    def get_results():
        """í˜„ì¬ ì €ì¥ëœ ëª¨ë“  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not hasattr(st.session_state, 'all_iteration_results'):
            st.session_state.all_iteration_results = []
        return st.session_state.all_iteration_results
    
    @staticmethod
    def get_current_iteration():
        """í˜„ì¬ ì„ íƒëœ ì´í„°ë ˆì´ì…˜ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not hasattr(st.session_state, 'current_iteration'):
            st.session_state.current_iteration = 0
        return st.session_state.current_iteration
    
    @staticmethod
    def set_current_iteration(iteration):
        """í˜„ì¬ ì´í„°ë ˆì´ì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        st.session_state.current_iteration = iteration

class ResultsDisplay:
    """
    ê²°ê³¼ í‘œì‹œë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self):
        SessionState.init_state()
        # ë©”ì¸ ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™”
        if 'main_container' not in st.session_state:
            st.session_state.main_container = st.empty()
    
    def display_metrics(self, results, container):
        """ì„±ëŠ¥ ì§€í‘œë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
        if not results:
            return
        
        # ê·¸ë˜í”„ ë°ì´í„° ì¤€ë¹„
        x_values = [result.iteration for result in results]
        avg_scores = [result.avg_score for result in results]
        best_sample_scores = [result.best_sample_score for result in results]
        std_devs = [result.std_dev for result in results]
        top3_scores = [result.top3_avg_score for result in results]
        
        
        # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
        category_scores = {
            'meaning_accuracy': [],
            'completeness': [],
            'expression_style': [],
            'faithfulness': [],
            'conciseness': [],
            'correctness': [],
            'structural_alignment': [],
            'reasoning_quality': []
        }
        
        for result in results:
            iteration_category_scores = {category: [] for category in category_scores.keys()}
            iteration_category_weights = {category: [] for category in category_scores.keys()}  # ê° ì´í„°ë ˆì´ì…˜ì˜ ê°€ì¤‘ì¹˜
            
            for test_case in result.test_case_results:
                if test_case.evaluation_details and 'category_scores' in test_case.evaluation_details:
                    for category, details in test_case.evaluation_details['category_scores'].items():
                        if category in iteration_category_scores:
                            iteration_category_scores[category].append(details['score'])
                            iteration_category_weights[category].append(details.get('weight', 0.5))  # ê°€ì¤‘ì¹˜ ì¶”ê°€
            
            # ê° ì¹´í…Œê³ ë¦¬ì˜ í‰ê·  ì ìˆ˜ì™€ ê°€ì¤‘ì¹˜ ì¶”ê°€
            for category in category_scores:
                scores = iteration_category_scores[category]
                weights = iteration_category_weights[category]
                avg_score = np.mean(scores) if scores else 0
                avg_weight = np.mean(weights) if weights else 0.5
                category_scores[category].append(avg_score)
        
        # í†µí•© ê·¸ë˜í”„ ìƒì„±
        fig = go.Figure()
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ë¥¼ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ì¶”ê°€
        for category in category_scores:
            fig.add_trace(go.Bar(
                x=x_values,
                y=category_scores[category],
                name=category,
                visible=True
            ))
        
        # ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ íŠ¸ë ˆì´ìŠ¤
        fig.add_trace(go.Scatter(
            x=x_values,
            y=avg_scores,
            name='í‰ê·  ì ìˆ˜',
            mode='lines+markers',
            line=dict(color='blue', width=2)
        ))
        
        fig.add_trace(go.Scatter(
            x=x_values,
            y=std_devs,
            name='í‘œì¤€í¸ì°¨',
            mode='lines+markers',
            line=dict(color='purple', width=2, dash='dot')
        ))
        
        fig.add_trace(go.Scatter(
            x=x_values,
            y=best_sample_scores,
            name='ìµœê³  ê°œë³„ ì ìˆ˜',
            mode='lines+markers',
            line=dict(color='green', width=2)
        ))
        
        fig.add_trace(go.Scatter(
            x=x_values,
            y=top3_scores,
            name='Top3 í‰ê·  ì ìˆ˜',
            mode='lines+markers',
            line=dict(color='red', width=2)
        ))
        
        # ê·¸ë˜í”„ ë ˆì´ì•„ì›ƒ ì„¤ì •
        fig.update_layout(
            title='í†µí•© ì„±ëŠ¥ ì§€í‘œ ë° ì¹´í…Œê³ ë¦¬ ë¶„ì„',
            xaxis_title='ì´í„°ë ˆì´ì…˜',
            yaxis_title='ì ìˆ˜',
            yaxis_range=[0, 1],
            xaxis=dict(
                tickmode='array',
                tickvals=x_values,
                ticktext=[f"Iteration {x}" for x in x_values]
            ),
            height=600,
            barmode='group',
            showlegend=True,
            legend=dict(
                yanchor="top",
                y=0.99,
                xanchor="left",
                x=1.05
            )
        )
        
        # ê·¸ë˜í”„ í‘œì‹œ
        container.plotly_chart(fig, use_container_width=True)
    
    def display_iteration_details(self, results, container):
        """ì´í„°ë ˆì´ì…˜ ìƒì„¸ ì •ë³´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
        if not results:
            container.info("ì•„ì§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì´í„°ë ˆì´ì…˜ ì„ íƒ
        total_iterations = len(results)
        if total_iterations > 0:
            # ì´í„°ë ˆì´ì…˜ ì„ íƒ UI
            current_iteration = SessionState.get_current_iteration()
            
            # ì´í„°ë ˆì´ì…˜ ì„ íƒì„ ìœ„í•œ íƒ­ ìƒì„±
            tabs = container.tabs([f"Iteration {i+1}" for i in range(total_iterations)])
            selected_iteration = current_iteration
            
            with tabs[selected_iteration]:
                iteration_result = results[selected_iteration]
                
                # í‰ê·  ì ìˆ˜ì™€ í‘œì¤€í¸ì°¨ í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                col1.metric("Average Score", f"{iteration_result.avg_score:.2f}")
                col2.metric("Standard Deviation", f"{iteration_result.std_dev:.2f}")
                col3.metric("Top 3 Average", f"{iteration_result.top3_avg_score:.2f}")
                
                # Task Typeê³¼ Description expander ì¶”ê°€
                with st.expander(f"Task Type ({iteration_result.task_type})", expanded=False):
                    st.markdown("### Task Description")
                    st.code(iteration_result.task_description, language="text")
                
                # í˜„ì¬ í”„ë¡¬í”„íŠ¸ expander ì¶”ê°€
                with st.expander("í˜„ì¬ í”„ë¡¬í”„íŠ¸ ë³´ê¸°", expanded=False):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown("### System Prompt")
                        st.code(iteration_result.system_prompt, language="text")
                    with col2:
                        st.markdown("### User Prompt")
                        st.code(iteration_result.user_prompt, language="text")
                
                # ê°€ì¤‘ì¹˜ ì ìˆ˜ expander ì¶”ê°€
                with st.expander("í˜„ì¬ ê°€ì¤‘ì¹˜ ì ìˆ˜ ë³´ê¸°", expanded=False):
                    # ê°€ì¤‘ì¹˜ ë°ì´í„° ìˆ˜ì§‘
                    weight_data = []
                    for test_case in iteration_result.test_case_results:
                        if test_case.evaluation_details and 'category_scores' in test_case.evaluation_details:
                            for category, details in test_case.evaluation_details['category_scores'].items():
                                weight = details.get('weight', 0.5)
                                weight_data.append({
                                    'Category': category,
                                    'Weight': weight
                                })
                    
                    if weight_data:
                        # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ê°€ì¤‘ì¹˜ ê³„ì‚°
                        df = pd.DataFrame(weight_data)
                        avg_weights = df.groupby('Category')['Weight'].mean().round(3)
                        avg_weights = avg_weights.reset_index()
                        avg_weights.columns = ['Category', 'Average Weight']
                        
                        # ë°ì´í„°í”„ë ˆì„ ìŠ¤íƒ€ì¼ë§
                        def highlight_weights(val):
                            color = f'background-color: rgba(255, 99, 71, {val})'
                            return color
                        
                        # ìŠ¤íƒ€ì¼ì´ ì ìš©ëœ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
                        st.write("Category Weights:")
                        styled_df = avg_weights.style.apply(lambda x: [highlight_weights(v) for v in x], subset=['Average Weight'])
                        st.dataframe(styled_df, use_container_width=True)
                
                # ì¶œë ¥ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
                outputs_data = []
                for i, test_case in enumerate(iteration_result.test_case_results):
                    row = {
                        'Test Case': i + 1,
                        'Score': f"{test_case.score:.2f}",
                        'Question': test_case.question,
                        'Expected': test_case.expected_output,
                        'Actual': test_case.actual_output,
                        'Evaluation Details': json.dumps(test_case.evaluation_details, ensure_ascii=False, indent=2)
                    }
                    
                    # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ì™€ í”¼ë“œë°± ì¶”ê°€
                    if test_case.evaluation_details and 'category_scores' in test_case.evaluation_details:
                        for category, details in test_case.evaluation_details['category_scores'].items():
                            row[f"{category} Score"] = f"{details['score']:.2f}"
                            row[f"{category} Weight"] = f"{details.get('weight', 1.0):.2f}"  # ê°€ì¤‘ì¹˜ í‘œì‹œ ì¶”ê°€
                            row[f"{category} State"] = details['current_state']
                            row[f"{category} Action"] = details['improvement_action']
                    
                    outputs_data.append(row)
                
                # ë°ì´í„°í”„ë ˆì„ ìƒì„±
                df = pd.DataFrame(outputs_data)
                
                # ë°ì´í„°í”„ë ˆì„ ìŠ¤íƒ€ì¼ë§ í•¨ìˆ˜
                def highlight_rows(df):
                    scores = df['Score'].astype(float)
                    max_score = scores.max()
                    min_score = scores.min()
                    
                    background_colors = pd.DataFrame('', index=df.index, columns=df.columns)
                    
                    max_score_mask = (scores == max_score)
                    min_score_mask = (scores == min_score)
                    
                    background_colors.loc[max_score_mask] = 'background-color: #90EE90'
                    background_colors.loc[min_score_mask] = 'background-color: #FFB6C6'
                    
                    return background_colors
                
                # ìŠ¤íƒ€ì¼ì´ ì ìš©ëœ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
                st.dataframe(
                    df.style.apply(highlight_rows, axis=None),
                    use_container_width=True,
                    height=400
                )
                
                # ë©”íƒ€í”„ë¡¬í”„íŠ¸ expander ì¶”ê°€
                if iteration_result.meta_prompt:
                    with st.expander("ë©”íƒ€í”„ë¡¬í”„íŠ¸ ê²°ê³¼ ë³´ê¸°", expanded=False):
                        st.code(iteration_result.meta_prompt, language="text")
            
            # í˜„ì¬ ì„ íƒëœ ì´í„°ë ˆì´ì…˜ ì €ì¥
            SessionState.set_current_iteration(selected_iteration)
    
    def update(self):
        """ê²°ê³¼ í‘œì‹œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        results = SessionState.get_results()
        if st.session_state.show_results and results:
            # ê¸°ì¡´ ì»¨í…Œì´ë„ˆë¥¼ ë¹„ìš°ê³  ìƒˆë¡œìš´ ì»¨í…Œì´ë„ˆ ìƒì„±
            with st.session_state.main_container.container():
                st.empty()  # ê¸°ì¡´ ë‚´ìš©ì„ ì§€ì›ë‹ˆë‹¤
                
                # ë©”íŠ¸ë¦­ìŠ¤ì™€ ìƒì„¸ ì •ë³´ë¥¼ í‘œì‹œí•  ìƒˆë¡œìš´ ì»¨í…Œì´ë„ˆ ìƒì„±
                metrics_container = st.container()
                details_container = st.container()
                
                # ë©”íŠ¸ë¦­ìŠ¤ì™€ ìƒì„¸ ì •ë³´ í‘œì‹œ
                self.display_metrics(results, metrics_container)
                self.display_iteration_details(results, details_container)

def run_tuning_process():
    """í”„ë¡¬í”„íŠ¸ íŠœë‹ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    # UI ìƒíƒœ ì´ˆê¸°í™”
    SessionState.init_state()
    results_display = ResultsDisplay()
    
    with st.spinner('í”„ë¡¬í”„íŠ¸ íŠœë‹ ì¤‘...'):
        def iteration_callback(result):
            logging.info(f"Iteration callback called for iteration {result.iteration}")
            SessionState.update_results(result)
            logging.info("Results updated, updating display...")
            results_display.update()
            logging.info("Display updated")
        
        # iteration_callback ì„¤ì •
        tuner.iteration_callback = iteration_callback
        
        # í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹¤í–‰
        tuner.tune_prompt(
            initial_system_prompt=system_prompt,
            initial_user_prompt=user_prompt,
            initial_test_cases=test_cases,
            num_iterations=iterations,
            score_threshold=score_threshold if use_threshold else None,
            evaluation_score_threshold=evaluation_threshold,
            use_meta_prompt=use_meta_prompt,
            num_samples=num_samples
        )
        
        st.session_state.tuning_complete = True
        logging.info("Tuning process completed")
        
        # ìµœì¢… ê²°ê³¼
        results = SessionState.get_results()
        if results:
            st.success("í”„ë¡¬í”„íŠ¸ íŠœë‹ ì™„ë£Œ!")
            logging.info(f"Final results count: {len(results)}")
            
            # ë¹„ìš© ìš”ì•½ í‘œì‹œ
            st.header("ğŸ’° ë¹„ìš© ë° ì‚¬ìš©ëŸ‰ ìš”ì•½")
            cost_summary = tuner.get_cost_summary()
            
            # ì „ì²´ ë¹„ìš© ì •ë³´ë¥¼ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‘œì‹œ
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì´ ë¹„ìš©", f"${cost_summary['total_cost']:.4f}")
            with col2:
                st.metric("ì´ í† í°", f"{cost_summary['total_tokens']:,}")
            with col3:
                st.metric("ì´ ì‹œê°„", f"{cost_summary['total_duration']:.1f}ì´ˆ")
            with col4:
                st.metric("ì´ í˜¸ì¶œ", f"{cost_summary['total_calls']}")
            
            # ëª¨ë¸ë³„ ìƒì„¸ ë¹„ìš© ì •ë³´
            with st.expander("ëª¨ë¸ë³„ ìƒì„¸ ë¹„ìš© ì •ë³´", expanded=False):
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.subheader("ğŸ¤– ëª¨ë¸ í˜¸ì¶œ")
                    model_stats = cost_summary['model_stats']
                    st.write(f"í˜¸ì¶œ íšŸìˆ˜: {model_stats['total_calls']}")
                    st.write(f"ì…ë ¥ í† í°: {model_stats['total_input_tokens']:,}")
                    st.write(f"ì¶œë ¥ í† í°: {model_stats['total_output_tokens']:,}")
                    st.write(f"ì´ í† í°: {model_stats['total_tokens']:,}")
                    st.write(f"ë¹„ìš©: ${model_stats['total_cost']:.4f}")
                    st.write(f"ì‹œê°„: {model_stats['total_duration']:.2f}ì´ˆ")
                
                with col2:
                    st.subheader("ğŸ“Š í‰ê°€ì í˜¸ì¶œ")
                    eval_stats = cost_summary['evaluator_stats']
                    st.write(f"í˜¸ì¶œ íšŸìˆ˜: {eval_stats['total_calls']}")
                    st.write(f"ì…ë ¥ í† í°: {eval_stats['total_input_tokens']:,}")
                    st.write(f"ì¶œë ¥ í† í°: {eval_stats['total_output_tokens']:,}")
                    st.write(f"ì´ í† í°: {eval_stats['total_tokens']:,}")
                    st.write(f"ë¹„ìš©: ${eval_stats['total_cost']:.4f}")
                    st.write(f"ì‹œê°„: {eval_stats['total_duration']:.2f}ì´ˆ")
                
                with col3:
                    st.subheader("ğŸ”§ ë©”íƒ€ í”„ë¡¬í”„íŠ¸ ìƒì„±")
                    meta_stats = cost_summary['meta_prompt_stats']
                    st.write(f"í˜¸ì¶œ íšŸìˆ˜: {meta_stats['total_calls']}")
                    st.write(f"ì…ë ¥ í† í°: {meta_stats['total_input_tokens']:,}")
                    st.write(f"ì¶œë ¥ í† í°: {meta_stats['total_output_tokens']:,}")
                    st.write(f"ì´ í† í°: {meta_stats['total_tokens']:,}")
                    st.write(f"ë¹„ìš©: ${meta_stats['total_cost']:.4f}")
                    st.write(f"ì‹œê°„: {meta_stats['total_duration']:.2f}ì´ˆ")
            
            # ì´í„°ë ˆì´ì…˜ë³„ ë¹„ìš© ë¶„ì„
            iteration_breakdown = tuner.get_iteration_cost_breakdown()
            if iteration_breakdown:
                with st.expander("ì´í„°ë ˆì´ì…˜ë³„ ë¹„ìš© ë¶„ì„", expanded=False):
                    # ì´í„°ë ˆì´ì…˜ë³„ ë¹„ìš© ë°ì´í„°ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
                    breakdown_data = []
                    for iteration_key, data in iteration_breakdown.items():
                        breakdown_data.append({
                            'Iteration': iteration_key.replace('iteration_', ''),
                            'Model Cost': f"${data['model_cost']:.4f}",
                            'Evaluator Cost': f"${data['evaluator_cost']:.4f}",
                            'Meta Prompt Cost': f"${data['meta_prompt_cost']:.4f}",
                            'Total Cost': f"${data['total_cost']:.4f}",
                            'Model Calls': data['model_calls'],
                            'Evaluator Calls': data['evaluator_calls'],
                            'Meta Prompt Calls': data['meta_prompt_calls'],
                            'Total Calls': data['total_calls']
                        })
                    
                    if breakdown_data:
                        df_breakdown = pd.DataFrame(breakdown_data)
                        st.dataframe(df_breakdown, use_container_width=True)
            
            # ì „ì²´ ê²°ê³¼ì—ì„œ ê°€ì¥ ë†’ì€ í‰ê·  ì ìˆ˜ë¥¼ ê°€ì§„ í”„ë¡¬í”„íŠ¸ ì°¾ê¸°
            st.header("ğŸ† ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸")
            best_result = max(results, key=lambda x: x.avg_score)
            st.write("Final Best Prompt:")
            col1, col2 = st.columns(2)
            with col1:
                st.write("System Prompt:")
                st.code(best_result.system_prompt)
            with col2:
                st.write("User Prompt:")
                st.code(best_result.user_prompt)
            st.write(f"ìµœì¢… ê²°ê³¼: í‰ê·  ì ìˆ˜ {best_result.avg_score:.2f}, ìµœê³  í‰ê·  ì ìˆ˜ {best_result.best_avg_score:.2f}, ìµœê³  ê°œë³„ ì ìˆ˜ {best_result.best_sample_score:.2f}")
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ë“¤
            st.header("ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")
            col1, col2 = st.columns(2)
            
            with col1:
                # ì „ì²´ ê²°ê³¼ (ë¹„ìš© ì •ë³´ í¬í•¨) CSV ë‹¤ìš´ë¡œë“œ
                try:
                    csv_data = tuner.save_results_to_csv()
                    st.download_button(
                        label="ğŸ“Š ì „ì²´ ê²°ê³¼ (ë¹„ìš© í¬í•¨) CSV ì €ì¥",
                        data=csv_data,
                        file_name=f"prompt_tuning_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv",
                        key="download_full_csv",
                        help="í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë³„ ìƒì„¸ ê²°ê³¼ì™€ ë¹„ìš© ì •ë³´ê°€ í¬í•¨ëœ ì „ì²´ ë°ì´í„°"
                    )
                except Exception as e:
                    st.error(f"ì „ì²´ ê²°ê³¼ CSV íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            
            with col2:
                # ë¹„ìš© ìš”ì•½ë§Œ CSV ë‹¤ìš´ë¡œë“œ
                try:
                    cost_csv_data = tuner.export_cost_summary_to_csv()
                    st.download_button(
                        label="ğŸ’° ë¹„ìš© ìš”ì•½ CSV ì €ì¥",
                        data=cost_csv_data,
                        file_name=f"cost_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv",
                        key="download_cost_csv",
                        help="ëª¨ë¸ë³„, ì´í„°ë ˆì´ì…˜ë³„ ë¹„ìš© ìš”ì•½ ë°ì´í„°"
                    )
                except Exception as e:
                    st.error(f"ë¹„ìš© ìš”ì•½ CSV íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            
            # ì½˜ì†”ì—ë„ ë¹„ìš© ìš”ì•½ ì¶œë ¥ (ê°œë°œììš©)
            tuner.print_cost_summary()
        else:
            st.warning("íŠœë‹ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# íŠœë‹ ì‹œì‘ ë²„íŠ¼
if st.button("í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹œì‘", type="primary"):
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    SessionState.reset()
    
    # API í‚¤ í™•ì¸
    required_keys = {
        "solar": "SOLAR_API_KEY",
        "gpt4o": "OPENAI_API_KEY",
        "claude": "ANTHROPIC_API_KEY",
        "local1": None,  # local1 ëª¨ë¸ì€ API í‚¤ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ
        "local2": None,   # local2 ëª¨ë¸ì€ API í‚¤ê°€ í•„ìš”í•˜ì§€ ì•ŠìŒ
        "solar_strawberry": "SOLAR_STRAWBERRY_API_KEY",  # ì¶”ê°€
    }
    
    # ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ë“¤ì˜ API í‚¤ í™•ì¸
    used_models = set([model_name, evaluator_model])
    missing_keys = []
    
    for model in used_models:
        key = required_keys[model]
        if key and not os.getenv(key):  # keyê°€ Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ API í‚¤ í™•ì¸
            missing_keys.append(f"{MODEL_INFO[model]['name']} ({key})")
    
    if missing_keys:
        st.error(f"ë‹¤ìŒ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤: {', '.join(missing_keys)}")
        st.info("API í‚¤ë¥¼ .env íŒŒì¼ì— ì„¤ì •í•˜ì„¸ìš”.")
    else:
        # ë©”íƒ€í”„ë¡¬í”„íŠ¸ê°€ ì…ë ¥ëœ ê²½ìš°ì—ë§Œ ì„¤ì •
        if meta_system_prompt.strip() and meta_user_prompt.strip():
            tuner.set_meta_prompt(meta_system_prompt, meta_user_prompt)
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì„¤ì •
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        def progress_callback(iteration, test_case_index):
            # í˜„ì¬ iterationì˜ ì§„í–‰ë„ (0ë¶€í„° ì‹œì‘)
            iteration_progress = (iteration - 1) / iterations
            # í˜„ì¬ test caseì˜ ì§„í–‰ë„ (0ë¶€í„° ì‹œì‘)
            test_case_progress = test_case_index / num_samples
            # ì „ì²´ ì§„í–‰ë„ ê³„ì‚°
            progress = iteration_progress + (test_case_progress / iterations)
            progress_bar.progress(progress)
            status_text.text(f"Iteration {iteration}/{iterations}, Test Case {test_case_index}/{num_samples}")
        
        tuner.progress_callback = progress_callback
        
        # í”„ë¡¬í”„íŠ¸ íŠœë‹ ì‹¤í–‰
        run_tuning_process() 