TASK_TYPE: 
{task_type}

TASK_DESCRIPTION:
{task_description}

Question:
{question}

Actual Output:
{response}

Expected Output:
{expected}

## Evaluation Requirements
Carefully assess the quality of the actual output using the eight criteria below. Use the expected output as a reference, but allow flexibility for well-reasoned alternatives that still fulfill the task requirements.

For each criterion, provide:
- A concise explanation (1–2 sentences) of how the output satisfies or falls short of the criterion
- One specific example from the actual output
- One concrete, actionable suggestion for improvement
- A score between 0.0 and 1.0
- A dynamic weight between 0.0 and 1.0 based on the importance of the criterion for the TASK_TYPE

Weighting Guidelines (based on TASK_TYPE):
- Classification: prioritize Correctness, Reasoning Quality
- Summarization: prioritize Faithfulness, Conciseness
- Code Generation: prioritize Correctness, Structural Alignment
- Math/Logic: prioritize Reasoning Quality, Correctness

Scoring Scale:
- 1.0 = Fully satisfies the criterion with no issues
- 0.7 = Minor issues but generally meets expectations
- 0.5 = Mixed quality or partially complete
- 0.2 = Major issues with some redeeming parts
- 0.0 = Completely incorrect, irrelevant, or missing

Maintain consistent logic and calibration across all evaluations. When comparing outputs to the expected output, evaluate only the final answer content—visible reasoning should be assessed separately under the Reasoning Quality criterion and should not affect scores like Correctness or Conciseness unless it introduces confusion or error. Ensure the final answer appears in the required format and position, typically on a separate final line, especially for fixed-format tasks.