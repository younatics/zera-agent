import streamlit as st
import pandas as pd
import plotly.express as px
from prompt_tuner import PromptTuner
import os
import logging
import plotly.graph_objects as go

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(page_title="Prompt Tuning Visualizer", layout="wide")

st.title("Prompt Tuning Dashboard")

# ëª¨ë¸ ì •ë³´ ì •ì˜
MODEL_INFO = {
    "solar": {
        "name": "Solar",
        "description": "Upstageì˜ Solar ëª¨ë¸",
        "version": "solar-pro"
    },
    "gpt4o": {
        "name": "GPT-4",
        "description": "OpenAIì˜ GPT-4 ëª¨ë¸",
        "version": "gpt-4"
    },
    "claude": {
        "name": "Claude",
        "description": "Anthropicì˜ Claude 3 Sonnet",
        "version": "claude-3-sonnet-20240229"
    }
}

# ê¸°ë³¸ í‰ê°€ í”„ë¡¬í”„íŠ¸
DEFAULT_EVALUATION_PROMPT = """ë‹¹ì‹ ì€ AI ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì‘ë‹µì´ ê¸°ëŒ€í•˜ëŠ” ì‘ë‹µê³¼ ì–¼ë§ˆë‚˜ ì˜ ì¼ì¹˜í•˜ëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

ì‹¤ì œ ì‘ë‹µ:
{response}

ê¸°ëŒ€í•˜ëŠ” ì‘ë‹µ:
{expected}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
1. ì˜ë¯¸ì  ìœ ì‚¬ì„± (ì‘ë‹µì´ ê¸°ëŒ€í•˜ëŠ” ë‚´ìš©ì„ ì–¼ë§ˆë‚˜ ì˜ ì „ë‹¬í•˜ëŠ”ê°€)
2. í†¤ê³¼ ìŠ¤íƒ€ì¼ (ì „ë¬¸ì ì´ê³  ê³µì†í•œ í†¤ì„ ìœ ì§€í•˜ëŠ”ê°€)
3. ì •ë³´ì˜ ì •í™•ì„± (ì˜ëª»ëœ ì •ë³´ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ”ê°€)
4. ì‘ë‹µì˜ ì™„ì„±ë„ (í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ”ê°€)

0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ì ìˆ˜ë§Œ ì¶œë ¥í•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
ì˜ˆì‹œ: 0.85"""

# ì‚¬ì´ë“œë°”ì—ì„œ íŒŒë¼ë¯¸í„° ì„¤ì •
with st.sidebar:
    st.header("íŠœë‹ íŒŒë¼ë¯¸í„°")
    iterations = st.slider("ë°˜ë³µ íšŸìˆ˜", min_value=1, max_value=10, value=3)
    
    # ëª¨ë¸ ì„ íƒ
    model_name = st.selectbox(
        "ëª¨ë¸ ì„ íƒ",
        options=list(MODEL_INFO.keys()),
        format_func=lambda x: f"{MODEL_INFO[x]['name']} ({MODEL_INFO[x]['version']})",
        help="í”„ë¡¬í”„íŠ¸ íŠœë‹ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”."
    )
    st.caption(MODEL_INFO[model_name]['description'])
    
    # í‰ê°€ ëª¨ë¸ ì„ íƒ
    evaluator_model = st.selectbox(
        "í‰ê°€ ëª¨ë¸ ì„ íƒ",
        options=list(MODEL_INFO.keys()),
        format_func=lambda x: f"{MODEL_INFO[x]['name']} ({MODEL_INFO[x]['version']})",
        help="ì‘ë‹µ í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”."
    )
    st.caption(MODEL_INFO[evaluator_model]['description'])

# í”„ë¡¬í”„íŠ¸ ì„¤ì •
with st.expander("í”„ë¡¬í”„íŠ¸ ì„¤ì •", expanded=False):
    # ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì…ë ¥
    initial_prompt = st.text_area(
        "ì´ˆê¸° í”„ë¡¬í”„íŠ¸ ì…ë ¥",
        value="You are a helpful AI assistant. Be polite and concise in your responses.",
        height=100,
        help="íŠœë‹ì„ ì‹œì‘í•  ì´ˆê¸° í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    )

    # í‰ê°€ í”„ë¡¬í”„íŠ¸ ì…ë ¥
    evaluation_prompt = st.text_area(
        "í‰ê°€ í”„ë¡¬í”„íŠ¸ ì…ë ¥",
        value=DEFAULT_EVALUATION_PROMPT,
        height=300,
        help="""ì‘ë‹µì„ í‰ê°€í•  ë•Œ ì‚¬ìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.
{response}ì™€ {expected}ëŠ” ì‹¤ì œ ì‘ë‹µê³¼ ê¸°ëŒ€ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤."""
    )

# CSV íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("Upload your CSV file", type=['csv'])

if uploaded_file is not None:
    try:
        # CSV íŒŒì¼ì„ ì½ì„ ë•Œ ë” ìœ ì—°í•œ íŒŒì‹± ì˜µì…˜ ì‚¬ìš©
        df = pd.read_csv(uploaded_file, 
                        encoding='utf-8',
                        on_bad_lines='skip',  # ë¬¸ì œê°€ ìˆëŠ” ì¤„ì€ ê±´ë„ˆë›°ê¸°
                        quoting=1,  # ëª¨ë“  í•„ë“œë¥¼ ë”°ì˜´í‘œë¡œ ê°ì‹¸ê¸°
                        escapechar='\\')  # ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì ì„¤ì •
        
        # ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
        st.write("Uploaded Data:")
        st.dataframe(df)
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
        test_cases = []
        for _, row in df.iterrows():
            test_cases.append({
                'input': row['question'],
                'expected_output': row['expected_answer']
            })
        
        # íŠœë‹ ì‹œì‘ ë²„íŠ¼
        if st.button("Start Prompt Tuning", type="primary"):
            # API í‚¤ í™•ì¸
            required_keys = {
                "solar": "SOLAR_API_KEY",
                "gpt4o": "OPENAI_API_KEY",
                "claude": "ANTHROPIC_API_KEY"
            }
            
            missing_keys = []
            for model in [model_name, evaluator_model]:
                key = required_keys[model]
                if not os.getenv(key):
                    missing_keys.append(f"{MODEL_INFO[model]['name']} ({key})")
            
            if missing_keys:
                st.error(f"ë‹¤ìŒ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤: {', '.join(missing_keys)}")
                st.info("API í‚¤ë¥¼ .env íŒŒì¼ì— ì„¤ì •í•˜ì„¸ìš”.")
            else:
                # í”„ë¡¬í”„íŠ¸ íŠœë„ˆ ì´ˆê¸°í™” ë° ì‹¤í–‰
                tuner = PromptTuner(model_name=model_name, evaluator_model_name=evaluator_model)
                tuner.set_evaluation_prompt(evaluation_prompt)
                
                with st.spinner("í”„ë¡¬í”„íŠ¸ íŠœë‹ ì¤‘..."):
                    results = tuner.tune(initial_prompt, test_cases, iterations=iterations)
                
                # ê²°ê³¼ í‘œì‹œ
                st.header("í”„ë¡¬í”„íŠ¸ íŠœë‹ ê²°ê³¼")
                
                # ìµœê³ ì˜ ê²°ê³¼ í‘œì‹œ
                best_result = max(results, key=lambda x: x['avg_score'])
                st.markdown("### ğŸ† ìµœê³ ì˜ ê²°ê³¼")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("í‰ê·  ì ìˆ˜", f"{best_result['avg_score']:.2f}")
                    st.metric("ìµœê³  ì ìˆ˜", f"{best_result['best_score']:.2f}")
                with col2:
                    st.metric("ìµœì € ì ìˆ˜", f"{best_result['worst_score']:.2f}")
                    st.metric("í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜", len(best_result['detailed_responses']))
                
                st.text_area("ìµœì  í”„ë¡¬í”„íŠ¸", value=best_result['prompt'], height=150)
                st.markdown("---")
                
                # Iterationë³„ë¡œ ê²°ê³¼ í‘œì‹œ
                for i, record in enumerate(results):
                    with st.expander(f"Iteration {i+1} (í‰ê·  ì ìˆ˜: {record['avg_score']:.2f})", expanded=True):
                        st.markdown(f"**í”„ë¡¬í”„íŠ¸:** {record['prompt']}")
                        st.markdown(f"**í‰ê·  ì ìˆ˜:** {record['avg_score']:.2f}")
                        st.markdown(f"**ìµœê³  ì ìˆ˜:** {record['best_score']:.2f}")
                        st.markdown(f"**ìµœì € ì ìˆ˜:** {record['worst_score']:.2f}")
                        st.markdown("---")
                        
                        # ì ìˆ˜ ë³€í™” ê·¸ë˜í”„
                        st.markdown("#### ì ìˆ˜ ë¶„í¬")
                        fig = go.Figure()
                        fig.add_trace(go.Box(
                            y=[r['score'] for r in record['detailed_responses']],
                            name='ì ìˆ˜ ë¶„í¬',
                            boxpoints='all',
                            jitter=0.3,
                            pointpos=-1.8
                        ))
                        fig.update_layout(
                            title='í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë³„ ì ìˆ˜ ë¶„í¬',
                            yaxis_title='ì ìˆ˜',
                            showlegend=False,
                            height=300
                        )
                        st.plotly_chart(fig, use_container_width=True, key=f"score_distribution_{i}")
                        
                        # ìƒì„¸ ê²°ê³¼ í‘œì‹œ
                        st.markdown("#### ìƒì„¸ ê²°ê³¼")
                        for j, response in enumerate(record['detailed_responses']):
                            st.markdown(f"##### í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {j+1} (ì ìˆ˜: {response['score']:.2f})")
                            st.text_area("ì§ˆë¬¸", value=response['input'], height=100, key=f"input_{i}_{j}")
                            st.text_area("ì‹¤ì œ ì‘ë‹µ", value=response['response'], height=150, key=f"response_{i}_{j}")
                            st.text_area("ê¸°ëŒ€ ì‘ë‹µ", value=response['expected'], height=150, key=f"expected_{i}_{j}")
                            st.markdown("---")  # êµ¬ë¶„ì„  ì¶”ê°€
                
    except Exception as e:
        st.error(f"Error processing CSV file: {str(e)}")
        logger.error(f"Error processing CSV file: {str(e)}") 